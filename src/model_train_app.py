import json
import logging
import os
import sys
import joblib
import pandas as pd
import numpy as np
import re
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.metrics import make_scorer, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, log_loss
from sklearn.feature_selection import SelectFromModel
import lightgbm as lgb
from hebo.design_space.design_space import DesignSpace
from hebo.optimizers.hebo import HEBO
import warnings
from pymoo.config import Config
Config.warnings['not_compiled'] = False

warnings.filterwarnings('ignore')

# å…¨å±€å­—ä½“è®¾ç½®ï¼šä¼˜å…ˆä½¿ç”¨ macOS å¸¸è§ä¸­æ–‡å­—ä½“ï¼Œé¿å…å›¾è¡¨ä¸­æ–‡å­—æ˜¾ç¤ºä¸ºå°æ–¹æ¡†
plt.rcParams['font.sans-serif'] = [
    'PingFang SC',        # macOS ç³»ç»Ÿä¸­æ–‡é»˜è®¤å­—ä½“
    'Hiragino Sans GB',   # è¾ƒæ–°çš„ä¸­æ–‡é»‘ä½“
    'Heiti TC',           # æ—§ç‰ˆé»‘ä½“
    'Songti SC',          # å®‹ä½“ç³»åˆ—
    'STHeiti',            # å…¼å®¹æ—©æœŸç³»ç»Ÿ
    'SimHei',             # Windows å¸¸è§é»‘ä½“
    'Arial Unicode MS',   # è·¨å¹³å°å¤‡ç”¨
    'DejaVu Sans'         # æœ€åå…œåº•
]
plt.rcParams['axes.unicode_minus'] = False

# ===================== å…¨å±€é…ç½® =====================
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

# ç¯å¢ƒé€‚é…
CURRENT_ENV = os.getenv("FOOTBALL_ENV", "dev")
if CURRENT_ENV == "prod":
    from config.prod_config import DB_PATH
else:
    from config.dev_config import DB_PATH

# ç›®å½•é…ç½®ï¼ˆæŒ‰ç¯å¢ƒåŒºåˆ†æ¨¡å‹ç›®å½•ï¼›metrics / visualization æš‚å…±ç”¨ï¼‰
if CURRENT_ENV == "prod":
    MODEL_DIR = os.path.join(PROJECT_ROOT, "trained_models")
else:
    # å¼€å‘ç¯å¢ƒå•ç‹¬ä½¿ç”¨ developed_models ç›®å½•ï¼Œé¿å…å’Œç”Ÿäº§æ¨¡å‹æ··åœ¨ä¸€èµ·
    MODEL_DIR = os.path.join(PROJECT_ROOT, "developed_models")

METRICS_DIR = os.path.join(PROJECT_ROOT, "metrics")
VIS_DIR = os.path.join(PROJECT_ROOT, "visualization")

# åˆ›å»ºç›®å½•
for dir_path in [MODEL_DIR, METRICS_DIR, VIS_DIR]:
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

# å¯¼å…¥å…¬å…±å·¥å…·å‡½æ•°
from utils import (
    load_historical_data, load_prediction_data,
    feature_engineering, init_model_pred_tables,
    get_model_historical_stats, save_prediction_to_db,
    get_db_connection, align_features_with_predictors,
    judge_prediction_hit  # ç¡®ä¿å¯¼å…¥å‘½ä¸­åˆ¤æ–­å‡½æ•°
)

# è¡¥å……åŸºç¡€æŒ‡æ ‡è®¡ç®—å‡½æ•°
def calculate_base_metrics(y_true, y_pred):
    return {
        'accuracy': round(accuracy_score(y_true, y_pred), 3),
        'precision': round(precision_score(y_true, y_pred), 3),
        'recall': round(recall_score(y_true, y_pred), 3),
        'f1': round(f1_score(y_true, y_pred), 3),
        'confusion_matrix': confusion_matrix(y_true, y_pred).tolist()
    }

# ===================== æ–°å¢ï¼šæ¨¡å‹è¿­ä»£ç®¡ç†é…ç½® =====================
BEST_MODEL_CONFIG = os.path.join(MODEL_DIR, "best_model_config.json")
MODEL_RETAIN_COUNT = 3  # ä¿ç•™æ¨¡å‹æ•°é‡ï¼š1ä¸ªæœ€ä¼˜ + 2ä¸ªæœ€æ–°

def load_best_model_config():
    """åŠ è½½æœ€ä¼˜æ¨¡å‹é…ç½®"""
    if not os.path.exists(BEST_MODEL_CONFIG):
        return {
            "model_date": "",
            "lgb_f1": 0.0,
            "lr_f1": 0.0,
            "lgb_path": "",
            "lr_path": "",
            "scaler_path": "",
            "lgb_features_path": "",
            "lr_features_path": "",
            "window_end": ""
        }
    with open(BEST_MODEL_CONFIG, "r", encoding="utf-8") as f:
        return json.load(f)

def save_best_model_config(config):
    """ä¿å­˜æœ€ä¼˜æ¨¡å‹é…ç½®"""
    with open(BEST_MODEL_CONFIG, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)

def get_recent_model_dates(top_n=2):
    """è·å–æœ€è¿‘è®­ç»ƒçš„nä¸ªæ¨¡å‹æ—¥æœŸï¼ˆæŒ‰æ—¶é—´é™åºï¼‰"""
    model_files = [f for f in os.listdir(MODEL_DIR) if f.startswith('lgb_model_')]
    # æå–æ¨¡å‹æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDD_HHMMSSï¼‰
    model_dates = [f.split('_')[2] + '_' + f.split('_')[3].split('.')[0] for f in model_files]
    # å»é‡+é™åºæ’åº
    model_dates = sorted(list(set(model_dates)), reverse=True)
    return model_dates[:top_n]

def clean_old_models():
    """æ¸…ç†å†—ä½™æ¨¡å‹ï¼šåªä¿ç•™æœ€ä¼˜+æœ€è¿‘2ä¸ªï¼ˆç¡®ä¿æ¨¡å‹ã€ç‰¹å¾ã€scaleræ–‡ä»¶åŒæ­¥ä¿ç•™ï¼‰"""
    best_config = load_best_model_config()
    recent_dates = get_recent_model_dates(top_n=2)

    # éœ€ä¿ç•™çš„æ¨¡å‹æ—¥æœŸï¼šæœ€ä¼˜æ¨¡å‹æ—¥æœŸ + æœ€è¿‘2ä¸ªæ¨¡å‹æ—¥æœŸï¼ˆå»é‡ï¼‰
    keep_dates = set(recent_dates)
    # ç¡®ä¿æœ€ä½³æ¨¡å‹æ—¥æœŸå­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
    best_model_date = best_config.get("model_date")
    if best_model_date:
        keep_dates.add(best_model_date)
        logger.info(f"æœ€ä½³æ¨¡å‹æ—¥æœŸ {best_model_date} å·²åŠ å…¥ä¿ç•™åˆ—è¡¨")
    keep_dates = list(keep_dates)
    logger.info(f"éœ€ä¿ç•™çš„æ¨¡å‹æ—¥æœŸï¼š{keep_dates}")

    # éå†æ‰€æœ‰æ¨¡å‹ç›¸å…³æ–‡ä»¶
    for root, _, files in os.walk(MODEL_DIR):
        for file in files:
            # åªå¤„ç†æ¨¡å‹ç›¸å…³æ–‡ä»¶ï¼ˆæ’é™¤å…¶ä»–æ— å…³æ–‡ä»¶ï¼‰
            if not any(file.startswith(prefix) for prefix in [
                'lgb_model_', 'lr_model_',
                'lgb_features_', 'lr_features_',
                'scaler_', 'model_predictors_', 'model_pred_type_'
            ]):
                continue

            # æ ¸å¿ƒä¿®å¤ï¼šæŒ‰æ–‡ä»¶ç±»å‹ä¿®æ­£æ—¥æœŸæå–ç´¢å¼•
            try:
                parts = file.split('_')
                if file.startswith(('lgb_model_', 'lr_model_', 'lgb_features_', 'lr_features_', 'model_predictors_')):
                    # æ ¼å¼ï¼šxxx_xxx_YYYYMMDD_HHMMSS.pkl â†’ å–ç´¢å¼•2ã€3
                    if len(parts) < 4:
                        raise IndexError("æ–‡ä»¶å‘½åæ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘è¶³å¤Ÿçš„ä¸‹åˆ’çº¿åˆ†å‰²éƒ¨åˆ†")
                    date_part1 = parts[2]
                    date_part2 = parts[3].split('.')[0]
                    file_date = f"{date_part1}_{date_part2}"
                elif file.startswith('model_pred_type_'):
                    # æ ¼å¼ï¼šmodel_pred_type_YYYYMMDD_HHMMSS.pkl â†’ å–ç´¢å¼•3ã€4
                    if len(parts) < 5:
                        raise IndexError("æ–‡ä»¶å‘½åæ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘è¶³å¤Ÿçš„ä¸‹åˆ’çº¿åˆ†å‰²éƒ¨åˆ†")
                    date_part1 = parts[3]
                    date_part2 = parts[4].split('.')[0]
                    file_date = f"{date_part1}_{date_part2}"
                elif file.startswith('scaler_'):
                    # æ ¼å¼ï¼šscaler_YYYYMMDD_HHMMSS.pkl â†’ å–ç´¢å¼•1ã€2
                    if len(parts) < 3:
                        raise IndexError("æ–‡ä»¶å‘½åæ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘è¶³å¤Ÿçš„ä¸‹åˆ’çº¿åˆ†å‰²éƒ¨åˆ†")
                    date_part1 = parts[1]
                    date_part2 = parts[2].split('.')[0]
                    file_date = f"{date_part1}_{date_part2}"
                else:
                    logger.warning(f"è·³è¿‡æœªçŸ¥æ–‡ä»¶ç±»å‹ï¼š{file}")
                    continue

                # éªŒè¯æ—¥æœŸæ ¼å¼ï¼ˆå¯é€‰ï¼Œå¢å¼ºå¥å£®æ€§ï¼‰
                if len(file_date.split('_')) != 2 or len(file_date.replace('_', '')) != 14:
                    logger.warning(f"æ–‡ä»¶ {file} çš„æ—¥æœŸæ ¼å¼ä¸æ­£ç¡®ï¼ˆåº”ä¸ºYYYYMMDD_HHMMSSï¼‰ï¼Œè·³è¿‡")
                    continue

            except Exception as e:
                logger.error(f"æå–æ–‡ä»¶ {file} æ—¥æœŸå¤±è´¥ï¼š{str(e)}ï¼Œè·³è¿‡è¯¥æ–‡ä»¶")
                continue

            # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ é™¤ï¼ˆä¸åœ¨ä¿ç•™æ—¥æœŸåˆ—è¡¨ä¸­ï¼‰
            if file_date not in keep_dates:
                file_path = os.path.join(root, file)
                try:
                    os.remove(file_path)
                    logger.info(f"åˆ é™¤å†—ä½™æ–‡ä»¶ï¼š{file_path}")
                except Exception as e:
                    logger.error(f"åˆ é™¤æ–‡ä»¶ {file_path} å¤±è´¥ï¼š{str(e)}")

# æ–°å¢ï¼šå…±çº¿æ€§å¤„ç†å‡½æ•°
def remove_high_correlation_features(X, threshold=0.8):
    """å‰”é™¤é«˜ç›¸å…³ç‰¹å¾ï¼ˆç›¸å…³ç³»æ•°ç»å¯¹å€¼>thresholdï¼‰"""
    corr_matrix = X.corr().abs()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [col for col in upper_triangle.columns if any(upper_triangle[col] > threshold)]
    X_cleaned = X.drop(columns=to_drop)
    return X_cleaned, to_drop

# é’ˆå¯¹æ€»è¿›çƒæ•°ç©æ³•å½“å‰ä¸åšä»»ä½•ç¡¬ç¼–ç çš„ç½®ä¿¡åº¦æƒ©ç½š
def adjust_goal_combo_confidence(df):
    """
    é’ˆå¯¹æ€»è¿›çƒæ•°ç©æ³•å½“å‰ä¸åšä»»ä½•ç¡¬ç¼–ç çš„ç½®ä¿¡åº¦æƒ©ç½šã€‚
    ä¿ç•™è¿™ä¸ªå‡½æ•°ä¸»è¦æ˜¯ä¸ºäº†æ¥å£å…¼å®¹ï¼Œåç»­å¦‚æœåŸºäºæ•°æ®åˆ†æéœ€è¦ï¼Œå¯ä»¥åœ¨è¿™é‡Œå®ç°æ•°æ®é©±åŠ¨çš„è°ƒæ•´é€»è¾‘ã€‚
    ç›®å‰ç›´æ¥è¿”å›åŸå§‹ç½®ä¿¡åº¦ã€‚
    """
    # å¦‚æœæ²¡æœ‰ confidence åˆ—ï¼Œç›´æ¥è¿”å›
    if 'confidence' not in df.columns:
        return df
    # å½“å‰ä¸åšè°ƒæ•´ï¼Œç›´æ¥è¿”å›åŸ DataFrame
    return df

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("model_train")

# ===================== 1. æ¨¡å‹è®­ç»ƒä¸è°ƒå‚ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šç‰¹å¾é€‰æ‹©ï¼‰ =====================
def hebo_lgb_tune(train_data, show_curve=True):
    """HEBOä¼˜åŒ–LightGBMå‚æ•°ï¼Œå¹¶åœ¨éœ€è¦æ—¶ç»˜åˆ¶loglossæ”¶æ•›æ›²çº¿"""
    param_config = [
        {'name': 'max_depth', 'type': 'num', 'lb': 3, 'ub': 6},
        {'name': 'num_leaves', 'type': 'int', 'lb': 16, 'ub': 32},
        {'name': 'learning_rate', 'type': 'num', 'lb': 0.01, 'ub': 0.08},
        {'name': 'reg_alpha', 'type': 'num', 'lb': 0.001, 'ub': 0.2},
        {'name': 'reg_lambda', 'type': 'num', 'lb': 0.001, 'ub': 0.2},
        # subsample / colsample æ›´é è¿‘ 1.0ï¼Œè®©æ¯æ£µæ ‘çœ‹æ›´å¤šæ ·æœ¬å’Œç‰¹å¾ï¼Œä»è€Œæœ‰æ›´å¤šæœºä¼šå°è¯•â€œäººç»´åº¦â€ç‰¹å¾
        {'name': 'subsample', 'type': 'num', 'lb': 0.8, 'ub': 1.0},
        {'name': 'colsample_bytree', 'type': 'num', 'lb': 0.8, 'ub': 1.0},
        # æ–°å¢ï¼šæ§åˆ¶å¶å­æœ€å°æ ·æœ¬æ•°ï¼Œè®©æ ‘æ›´æ„¿æ„åœ¨ç¨€ç–çš„äººç‰¹å¾ä¸Šç»§ç»­åˆ‡åˆ†
        {'name': 'min_data_in_leaf', 'type': 'int', 'lb': 10, 'ub': 80},
    ]

    space = DesignSpace()
    space.parse(param_config)

    def objective(params):
        params = params.iloc[0].to_dict()
        int_params = ['num_leaves', 'max_depth', 'min_data_in_leaf']
        for param in int_params:
            if param in params:
                params[param] = int(params[param])

        params.update({
            'objective': 'binary',
            'metric': 'binary_logloss',
            'verbose': -1,
            'seed': 42,
            'feature_pre_filter': False
        })
        cv_results = lgb.cv(
            params,
            train_data,
            num_boost_round=200,
            nfold=3,
            stratified=True,
        )

        target_key = 'valid binary_logloss-mean'
        if target_key in cv_results and len(cv_results[target_key]) > 0:
            vals = cv_results[target_key]
            return float(np.min(vals))
        else:
            return 1e9

    hebo = HEBO(space, model_name='gp')
    for _ in range(20):
        try:
            suggest = hebo.suggest()
            loss = objective(suggest)
            hebo.observe(suggest, np.array([loss]))
            print(f"HEBO iteration {_ + 1}, Suggested params: {suggest.to_dict()}, Loss: {loss:.4f}")
        except Exception as e:
            print(f"HEBO iteration {_ + 1} failed with error: {e}")
            continue

    best_params = hebo.best_x.iloc[0].to_dict()
    int_params = ['num_leaves', 'max_depth', 'min_data_in_leaf']
    for param in int_params:
        if param in best_params:
            best_params[param] = int(best_params[param])
    # æ˜¾å¼åŒæ­¥ feature_fractionï¼Œé¼“åŠ±æ¯æ£µæ ‘ä½¿ç”¨æ›´å¤šç‰¹å¾
    if 'colsample_bytree' in best_params and 'feature_fraction' not in best_params:
        best_params['feature_fraction'] = best_params['colsample_bytree']

    best_params.update({
        'objective': 'binary',
        'metric': 'binary_logloss',
        'verbose': -1,
        'seed': 42,
        'bagging_freq': 5,
        'feature_fraction_seed': 42,
        'bagging_seed': 42,
    })

    # ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°åšä¸€éCVï¼Œè®°å½•loglosséšè¿­ä»£è½®æ•°çš„å˜åŒ–ï¼Œå¹¶æ‰¾åˆ°æœ€ä¼˜ boosting è½®æ•°
    logloss_curve = None
    best_boost_round = 200  # é»˜è®¤ä¸º 200 è½®ï¼Œè‹¥CVæˆåŠŸåˆ™ç”¨æœ€ä¼˜è½®æ•°è¦†ç›–
    try:
        cv_results = lgb.cv(
            best_params,
            train_data,
            num_boost_round=200,
            nfold=3,
            stratified=True,
        )
        target_key = 'valid binary_logloss-mean'
        if target_key in cv_results and len(cv_results[target_key]) > 0:
            logloss_curve = cv_results[target_key]
            # æ–¹æ¡ˆAï¼šä½¿ç”¨æ•´ä¸ªæ›²çº¿ä¸­çš„æœ€å° logloss å¯¹åº”çš„è½®æ•°ä½œä¸ºæœ€ä¼˜ boosting è½®æ•°
            best_boost_round = int(np.argmin(logloss_curve)) + 1
    except Exception as e:
        logger.warning(f"LightGBM CVç»˜åˆ¶loglossæ›²çº¿å¤±è´¥: {e}")

    if show_curve and logloss_curve is not None:
        st.markdown("### ğŸ“‰ LightGBM CV Logloss æ”¶æ•›æ›²çº¿ï¼ˆæœ€ä½³å‚æ•°ï¼‰")
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.plot(range(1, len(logloss_curve) + 1), logloss_curve, marker='o', linewidth=1)
        ax.set_xlabel('è¿­ä»£è½®æ•°ï¼ˆnum_boost_roundï¼‰', fontsize=12)
        ax.set_ylabel('éªŒè¯é›† logloss', fontsize=12)
        ax.set_title('CV Logloss vs Boosting Round', fontsize=14, fontweight='bold')
        ax.grid(alpha=0.3)
        st.pyplot(fig)

    return best_params, best_boost_round

def train_base_models(X, y, current_predictor_ids, prediction_type):
    """è®­ç»ƒæµç¨‹ï¼šæ”¯æŒæ–°å¢é¢„æµ‹è€…ï¼Œä¿å­˜é¢„æµ‹è€…IDå’Œç‰¹å¾æ¨¡æ¿"""
    from lightgbm import LGBMClassifier

    # åˆ†å±‚æŠ½æ ·åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†ï¼ˆ8:2ï¼‰
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # ---------------------- æ­¥éª¤1-4ï¼šåŸè®­ç»ƒé€»è¾‘ä¿ç•™ï¼Œæ–°å¢ç‰¹å¾ç­›é€‰æ—¶è€ƒè™‘æ‰€æœ‰é¢„æµ‹è€… ----------------------
    sample_weight = np.array([2 if lbl == 1 else 1 for lbl in y_train])
    train_data = lgb.Dataset(X_train, label=y_train, weight=sample_weight)
    best_lgb_params, best_boost_round = hebo_lgb_tune(train_data, show_curve=True)

    # ä¸´æ—¶æ¨¡å‹è·å–ç‰¹å¾é‡è¦æ€§
    lgb_clf_temp = LGBMClassifier(**best_lgb_params, n_estimators=best_boost_round, random_state=42)
    lgb_clf_temp.fit(X_train, y_train, sample_weight=sample_weight, eval_set=[(X_test, y_test)], eval_metric='binary_logloss')

    # æå–ç‰¹å¾é‡è¦æ€§ï¼ˆè¦†ç›–æ‰€æœ‰é¢„æµ‹è€…çš„ç‰¹å¾ï¼‰ï¼Œä»…ç”¨äºåç»­å¯è§†åŒ–å’Œåˆ†æ
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': lgb_clf_temp.feature_importances_
    }).sort_values('importance', ascending=False)

    # ä½¿ç”¨æ‰€æœ‰ç‰¹å¾ä½œä¸º LightGBM çš„è®­ç»ƒç‰¹å¾ï¼ˆå½“å‰å·¥ç¨‹ç‰¹å¾ç»´åº¦æœ¬èº«ä¸é«˜ï¼‰
    all_lgb_features = X.columns.tolist()

    # å‰”é™¤é«˜ç›¸å…³ç‰¹å¾ï¼ˆä»…å¯¹ LR åšä¸€å±‚å…±çº¿æ€§å¤„ç†ï¼‰
    X_train_lr = X_train.copy()
    X_test_lr = X_test.copy()
    X_train_lr_cleaned, dropped_cols = remove_high_correlation_features(X_train_lr)
    X_test_lr_cleaned = X_test_lr.drop(columns=dropped_cols)
    final_lr_features = X_train_lr_cleaned.columns.tolist()

    # é‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    X_train_lgb = X_train[all_lgb_features].copy()
    X_test_lgb = X_test[all_lgb_features].copy()

    lgb_clf_final = LGBMClassifier(**best_lgb_params, n_estimators=best_boost_round, random_state=42)
    lgb_clf_final.fit(
        X_train_lgb,
        y_train,
        sample_weight=sample_weight,
        eval_set=[(X_test_lgb, y_test)],
        eval_metric='binary_logloss'
    )

    scaler = StandardScaler()
    X_train_lr_scaled = scaler.fit_transform(X_train_lr_cleaned)
    X_test_lr_scaled = scaler.transform(X_test_lr_cleaned)

    lr_model = LogisticRegression(
        penalty='l2',
        C=1.0,
        solver='liblinear',
        class_weight='balanced',
        random_state=42
    )
    lr_model.fit(X_train_lr_scaled, y_train)

    # ---------------------- æ­¥éª¤5ï¼šä¿å­˜æ¨¡å‹æ—¶æ–°å¢é¢„æµ‹è€…IDå’Œç‰¹å¾æ¨¡æ¿ ----------------------
    model_date = datetime.now().strftime("%Y%m%d_%H%M%S")
    # ä¿å­˜æ ¸å¿ƒæ¨¡å‹æ–‡ä»¶
    joblib.dump(lr_model, os.path.join(MODEL_DIR, f'lr_model_{model_date}.pkl'))
    joblib.dump(lgb_clf_final, os.path.join(MODEL_DIR, f'lgb_model_{model_date}.pkl'))
    joblib.dump(scaler, os.path.join(MODEL_DIR, f'scaler_{model_date}.pkl'))
    joblib.dump(final_lr_features, os.path.join(MODEL_DIR, f'lr_features_{model_date}.pkl'))
    joblib.dump(all_lgb_features, os.path.join(MODEL_DIR, f'lgb_features_{model_date}.pkl'))
    # æ–°å¢ï¼šä¿å­˜è®­ç»ƒæ—¶çš„é¢„æµ‹è€…IDå’Œé¢„æµ‹ç±»å‹ï¼ˆç”¨äºåç»­å¯¹é½ï¼‰
    joblib.dump(current_predictor_ids, os.path.join(MODEL_DIR, f'model_predictors_{model_date}.pkl'))
    joblib.dump(prediction_type, os.path.join(MODEL_DIR, f'model_pred_type_{model_date}.pkl'))

    # è®¡ç®—æŒ‡æ ‡ï¼ˆåŸé€»è¾‘ä¿ç•™ï¼‰
    lgb_pred = lgb_clf_final.predict(X_test_lgb)
    lr_pred = lr_model.predict(X_test_lr_scaled)
    lr_metrics = calculate_base_metrics(y_test, lr_pred)
    lgb_metrics = calculate_base_metrics(y_test, lgb_pred)

    # æ¦‚ç‡è¾“å‡ºï¼šç”¨äºåç»­ logloss / bucket åˆ†æ & LGB vs Ensemble å¯¹æ¯”
    lr_proba = lr_model.predict_proba(X_test_lr_scaled)[:, 1]
    lgb_proba = lgb_clf_final.predict_proba(X_test_lgb)[:, 1]
    ensemble_proba = (lgb_proba + lr_proba) / 2

    # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡ï¼ˆå¢åŠ NaNæ£€æŸ¥ï¼‰
    def get_stability_metrics(scores):
        scores = scores[~np.isnan(scores)]  # å»é™¤NaNå€¼
        if len(scores) == 0:
            return 0.0, 0.0
        return round(scores.mean(), 3), round(scores.std(), 3)

    # ç¨³å®šæ€§æŒ‡æ ‡ï¼ˆåŸé€»è¾‘ä¿ç•™ï¼‰
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    def safe_f1_score(y_true, y_pred): return f1_score(y_true, y_pred) if len(np.unique(y_true)) > 1 else 0.0
    lgb_cv_scores = cross_val_score(
        LGBMClassifier(**best_lgb_params, n_estimators=best_boost_round, random_state=42),
        X[all_lgb_features],
        y,
        cv=skf,
        scoring=make_scorer(safe_f1_score)
    )
    lr_cv_scores = cross_val_score(LogisticRegression(**lr_model.get_params()), scaler.transform(X[final_lr_features]), y, cv=skf, scoring=make_scorer(safe_f1_score))
    lr_mean, lr_var = get_stability_metrics(lr_cv_scores)
    lgb_mean, lgb_var = get_stability_metrics(lgb_cv_scores)

    stability_metrics = {
        'lr_cv_f1_mean': lr_mean, 'lr_cv_var': lr_var,
        'lgb_cv_f1_mean': lgb_mean, 'lgb_cv_var': lgb_var,
        'lgb_top300_features': all_lgb_features,  # ç°åœ¨å«ä¹‰æ˜¯â€œLightGBMå®é™…ä½¿ç”¨çš„å…¨éƒ¨ç‰¹å¾â€
        'lr_final_features': final_lr_features,
        'model_predictors': current_predictor_ids, 'prediction_type': prediction_type
    }

    metrics = {
        'lr_metrics': lr_metrics, 'lgb_metrics': lgb_metrics, 'stability_metrics': stability_metrics,
        'train_date': model_date, 'feature_cols': X.columns.tolist(),
        'model_predictors': current_predictor_ids, 'prediction_type': prediction_type
    }
    joblib.dump(metrics, os.path.join(METRICS_DIR, f'metrics_{model_date}.pkl'))

    # è¿”å›ç»“æœï¼ˆæ–°å¢é¢„æµ‹è€…IDå’Œé¢„æµ‹ç±»å‹ï¼‰
    return {
        'lr_model': lr_model, 'lgb_model': lgb_clf_final, 'scaler': scaler,
        'metrics': metrics, 'X_test': X_test, 'y_test': y_test,
        'lr_pred': lr_pred,
        'lgb_pred': lgb_pred,
        'lgb_proba': lgb_proba,
        'lr_proba': lr_proba,
        'ensemble_proba': ensemble_proba,
        'lgb_top300_features': all_lgb_features, 'lr_final_features': final_lr_features,
        'model_date': model_date, 'model_predictors': current_predictor_ids, 'prediction_type': prediction_type
    }

# ===================== æ ¸å¿ƒä¿®æ”¹ï¼šå¾®è°ƒæ¨¡å‹å‡½æ•°ï¼ˆé€‚é…ç‰¹å¾å¯¹é½ï¼‰ =====================
def fine_tune_model(prev_model_path, prev_scaler_path, prev_lr_features, prev_lgb_features, X_new_lr, X_new_lgb, y_new):
    """å¾®è°ƒæ¨¡å‹ï¼ˆæ¥æ”¶å¯¹é½åçš„LR/LGBç‰¹å¾ï¼Œç¡®ä¿ä¸å†å²æ¨¡å‹ä¸€è‡´ï¼‰"""
    # åŠ è½½å†å²æ¨¡å‹å’Œscaler
    lr_model = joblib.load(prev_model_path)
    lgb_model = joblib.load(prev_model_path.replace('lr_', 'lgb_'))
    scaler = joblib.load(prev_scaler_path)

    # ä¸¥æ ¼éªŒè¯ç‰¹å¾ä¸€è‡´æ€§ï¼ˆé¿å…å¯¹é½é—æ¼ï¼‰
    assert list(X_new_lr.columns) == prev_lr_features, f"LRç‰¹å¾ä¸ä¸€è‡´ï¼šå†å²{len(prev_lr_features)}ä¸ªï¼Œå½“å‰{len(X_new_lr.columns)}ä¸ª"
    assert list(X_new_lgb.columns) == prev_lgb_features, f"LGBç‰¹å¾ä¸ä¸€è‡´ï¼šå†å²{len(prev_lgb_features)}ä¸ªï¼Œå½“å‰{len(X_new_lgb.columns)}ä¸ª"

    # æ ‡å‡†åŒ–æ–°æ•°æ®ï¼ˆLRï¼‰
    X_new_lr_scaled = scaler.transform(X_new_lr).astype(np.float64)

    # å¾®è°ƒLogistic Regressionï¼ˆSGDï¼‰
    sgd_model = SGDClassifier(
        loss='log_loss',
        penalty='l2',
        alpha=1.0,
        random_state=42,
        warm_start=True,
        learning_rate='constant',
        eta0=0.01,
        max_iter=100
    )
    if hasattr(lr_model, 'coef_') and hasattr(lr_model, 'intercept_'):
        sgd_model.coef_ = lr_model.coef_
        sgd_model.intercept_ = lr_model.intercept_
    sgd_model.partial_fit(X_new_lr_scaled, y_new, classes=[0, 1])

    # å¾®è°ƒLightGBM
    lgb_model.set_params(learning_rate=0.05)
    sample_weight = np.array([2 if lbl == 1 else 1 for lbl in y_new])
    lgb_model.fit(
        X_new_lgb,
        y_new,
        sample_weight=sample_weight,
    )

    # è¯„ä¼°å¾®è°ƒåæ¨¡å‹
    lr_pred = sgd_model.predict(X_new_lr_scaled)
    lgb_pred = lgb_model.predict(X_new_lgb)
    lgb_proba = lgb_model.predict_proba(X_new_lgb)[:, 1]

    # è®¡ç®—æŒ‡æ ‡
    lr_metrics = calculate_base_metrics(y_new, lr_pred)
    lgb_metrics = calculate_base_metrics(y_new, lgb_pred)

    # ç¨³å®šæ€§æŒ‡æ ‡
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    def safe_f1_score(y_true, y_pred):
        try:
            return f1_score(y_true, y_pred)
        except:
            return 0.0

    # LRç¨³å®šæ€§è¯„ä¼°
    lr_cv_scores = cross_val_score(sgd_model, X_new_lr_scaled, y_new, cv=skf, scoring=make_scorer(safe_f1_score))
    # LightGBMç¨³å®šæ€§è¯„ä¼°ï¼ˆç”¨æ¨¡å‹è“å›¾ï¼‰
    lgb_params = lgb_model.get_params()
    lgb_cv_scores = cross_val_score(
        estimator=lgb.LGBMClassifier(**lgb_params),
        X=X_new_lgb,
        y=y_new,
        cv=skf,
        scoring=make_scorer(safe_f1_score),
        fit_params={'sample_weight': sample_weight}
    )

    # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡ï¼ˆå¢åŠ NaNæ£€æŸ¥ï¼‰
    def get_stability_metrics(scores):
        scores = scores[~np.isnan(scores)]
        if len(scores) == 0:
            return 0.0, 0.0
        return round(scores.mean(), 3), round(scores.std(), 3)

    lr_mean, lr_var = get_stability_metrics(lr_cv_scores)
    lgb_mean, lgb_var = get_stability_metrics(lgb_cv_scores)

    stability_metrics = {
        'lr_cv_f1_mean': lr_mean,
        'lr_cv_var': lr_var,
        'lgb_cv_f1_mean': lgb_mean,
        'lgb_cv_var': lgb_var,
        'lgb_top300_features': prev_lgb_features,  # æ²¿ç”¨å†å²ç‰¹å¾åˆ—è¡¨
        'lr_final_features': prev_lr_features       # æ²¿ç”¨å†å²ç‰¹å¾åˆ—è¡¨
    }

    return {
        'lr_model': sgd_model,
        'lgb_model': lgb_model,
        'scaler': scaler,
        'lr_metrics': lr_metrics,
        'lgb_metrics': lgb_metrics,
        'stability_metrics': stability_metrics,
        'lr_pred': lr_pred,
        'lgb_pred': lgb_pred,
        'lgb_proba': lgb_proba,
        'lgb_top300_features': prev_lgb_features,
        'lr_final_features': prev_lr_features
    }

# ===================== 2. å…¨é‡è®­ç»ƒæµç¨‹ï¼ˆæ›¿ä»£æ»šåŠ¨è®­ç»ƒï¼‰ =====================
def train_global_model(start_date, end_date):
    """
    ä½¿ç”¨æŒ‡å®šæ—¶é—´åŒºé—´å†…çš„å…¨é‡å†å²æ•°æ®è¿›è¡Œè®­ç»ƒä¸è¯„ä¼°ï¼š
    - å…ˆæŒ‰ betting_cycle_date åˆ’åˆ†ä¸ºè‹¥å¹²æ—¶é—´çª—å£ï¼Œå¯¹æ¯ä¸ªçª—å£å•ç‹¬è®­ç»ƒ/éªŒè¯ï¼Œåšâ€œæ—¶åºåˆ‡ç‰‡åˆ†æâ€ï¼›
    - ç„¶åä»ç„¶ä½¿ç”¨å…¨é‡æ•°æ®åšä¸€æ¬¡æ€§è®­ç»ƒï¼Œä¿æŒåŸæœ‰ best model ä¿å­˜é€»è¾‘ä¸å˜ã€‚
    """
    logger.info("===== å¼€å§‹æ‰§è¡Œå…¨é‡è®­ç»ƒ =====")
    init_model_pred_tables(DB_PATH)

    # 1. åŠ è½½å…¨é‡å†å²æ•°æ®

    # 1. åŠ è½½å…¨é‡å†å²æ•°æ®
    full_df = load_historical_data(DB_PATH, start_date, end_date)
    if full_df.empty:
        st.error("âŒ æœªåŠ è½½åˆ°æœ‰æ•ˆè®­ç»ƒæ•°æ®")
        return pd.DataFrame()
    full_df = full_df.sort_values('betting_cycle_date').reset_index(drop=True)
    min_date = full_df['betting_cycle_date'].min()
    max_date = full_df['betting_cycle_date'].max()

    # è®­ç»ƒå†å²è®°å½•ï¼ˆè¿™é‡Œä»ç„¶ç”¨ä¸åŸæ¥æ»šåŠ¨è®­ç»ƒç›¸åŒçš„å­—æ®µï¼Œæ–¹ä¾¿åç»­å¤ç”¨å¯è§†åŒ–ï¼‰
    train_history = pd.DataFrame(columns=[
        'window_start', 'window_end', 'sample_count', 'valid_predictors_count',
        'lr_f1', 'lgb_f1', 'lr_cv_var', 'lgb_cv_var', 'best_model', 'is_new_best'
    ])

    # å½“å‰æœ€ä¼˜æ¨¡å‹ä¿¡æ¯å±•ç¤º
    best_config = load_best_model_config()
    st.sidebar.markdown("### ğŸ† å½“å‰æœ€ä¼˜æ¨¡å‹")
    st.sidebar.info(f"""
    è®­ç»ƒçª—å£ï¼š{best_config['window_end'] or 'æ— '}
    æ¨¡å‹æ—¥æœŸï¼š{best_config['model_date'] or 'æ— '}
    LightGBM F1ï¼š{best_config['lgb_f1']:.3f}
    é€»è¾‘å›å½’ F1ï¼š{best_config['lr_f1']:.3f}
    """)

    # 2. åªä¿ç•™æœ‰èµ›æœçš„æ¯”èµ›
    required_result_cols = ['home_goals', 'away_goals']
    if not all(col in full_df.columns for col in required_result_cols):
        missing_cols = [col for col in required_result_cols if col not in full_df.columns]
        st.error(f"âŒ å…¨é‡æ•°æ®ç¼ºå°‘å¿…è¦çš„èµ›æœåˆ—: {missing_cols}ï¼Œæ— æ³•è®­ç»ƒã€‚")
        return train_history

    full_df = full_df[
        (full_df['home_goals'].notna()) & (full_df['home_goals'] != '') &
        (full_df['away_goals'].notna()) & (full_df['away_goals'] != '')
    ].copy()

    if len(full_df) < 50:
        st.warning(f"âš ï¸ å…¨é‡æœ‰æ•ˆæ•°æ®é‡è¿‡å°‘ï¼ˆ{len(full_df)}æ¡ï¼‰ï¼Œä¸è¶³ä»¥è¿›è¡Œè®­ç»ƒã€‚")
        return train_history

    # ==================== 3. æŒ‰æ—¶é—´çª—å£åˆ‡åˆ† + çª—å£çº§è®­ç»ƒ/éªŒè¯ ====================
    st.markdown("### â± æŒ‰æ—¶é—´çª—å£çš„æ¨¡å‹è¡¨ç°åˆ†æ")

    # 3.1 æ„é€ æ—¶é—´çª—å£ï¼ˆæŒ‰ betting_cycle_date çš„å”¯ä¸€æ—¥æœŸåˆ‡æˆæœ€å¤š 4 æ®µï¼‰
    full_df = full_df.sort_values('betting_cycle_date').reset_index(drop=True)
    date_series = full_df['betting_cycle_date'].dt.date
    unique_dates = sorted(date_series.unique())

    window_logs = []
    bucket_log_rows = []
    human_imp_list = []

    if len(unique_dates) >= 3:
        max_windows = 4
        # è‡³å°‘ 3 æ®µï¼Œæœ€å¤š 4 æ®µï¼Œä½†ä¸èƒ½è¶…è¿‡æ—¥æœŸæ€»æ•°
        n_windows = min(max_windows, len(unique_dates))
        if n_windows < 3:
            n_windows = max(1, n_windows)
        date_splits = np.array_split(unique_dates, n_windows)

        for idx, date_chunk in enumerate(date_splits):
            if len(date_chunk) == 0:
                continue

            win_start = date_chunk[0]
            win_end = date_chunk[-1]

            window_mask = date_series.isin(date_chunk)
            df_win = full_df[window_mask].copy()

            if len(df_win) < 30:
                st.info(f"âš ï¸ æ—¶é—´çª—å£ {idx + 1}ï¼ˆ{win_start} ~ {win_end}ï¼‰æ ·æœ¬æ•°è¿‡å°‘ï¼ˆ{len(df_win)}ï¼‰ï¼Œè·³è¿‡çª—å£è®­ç»ƒã€‚")
                continue

            try:
                X_w, y_w, feature_cols_w, current_predictor_ids_w = feature_engineering(df_win, is_training=True)
                valid_predictors_count_w = len(current_predictor_ids_w)
            except Exception as e:
                st.warning(f"âš ï¸ æ—¶é—´çª—å£ {idx + 1}ï¼ˆ{win_start} ~ {win_end}ï¼‰ç‰¹å¾å·¥ç¨‹å¤±è´¥ï¼š{str(e)}")
                continue

            current_pred_type_w = df_win['prediction_type'].iloc[0]
            st.write(f"ğŸ”§ çª—å£ {idx + 1} è®­ç»ƒï¼š{win_start} ~ {win_end}ï¼Œæ ·æœ¬ {len(df_win)}ï¼Œé¢„æµ‹è€… {valid_predictors_count_w}")

            # ä½¿ç”¨çª—å£å†…æ•°æ®ä»é›¶å¼€å§‹è®­ç»ƒæ¨¡å‹
            train_result_w = train_base_models(X_w, y_w, current_predictor_ids_w, current_pred_type_w)

            # 3.2 è®¡ç®—è¯¥çª—å£çš„ loglossï¼ˆéªŒè¯é›†ï¼‰
            y_test_w = train_result_w['y_test']
            lgb_proba_w = train_result_w['lgb_proba']
            try:
                lgb_logloss_w = log_loss(y_test_w, lgb_proba_w)
            except Exception:
                lgb_logloss_w = np.nan

            lgb_f1_w = train_result_w['metrics']['lgb_metrics']['f1']
            lr_f1_w = train_result_w['metrics']['lr_metrics']['f1']
            lr_cv_var_w = train_result_w['metrics']['stability_metrics']['lr_cv_var']
            lgb_cv_var_w = train_result_w['metrics']['stability_metrics']['lgb_cv_var']

            window_logs.append({
                'window_idx': idx + 1,
                'window_start': str(win_start),
                'window_end': str(win_end),
                'sample_count': len(df_win),
                'valid_predictors_count': valid_predictors_count_w,
                'lr_f1': lr_f1_w,
                'lgb_f1': lgb_f1_w,
                'lr_cv_var': lr_cv_var_w,
                'lgb_cv_var': lgb_cv_var_w,
                'lgb_logloss': lgb_logloss_w
            })

            # 3.3 å„ bucket é«˜ç½®ä¿¡å‘½ä¸­ç‡ï¼ˆåŸºäºè¯¥çª—å£éªŒè¯é›†ï¼‰
            y_array_w = np.asarray(y_test_w)
            high_thresholds = [0.6, 0.7, 0.8, 0.9]
            for th in high_thresholds:
                mask = lgb_proba_w >= th
                selected = int(mask.sum())
                if selected > 0:
                    hit_rate = float(y_array_w[mask].mean())
                else:
                    hit_rate = np.nan
                bucket_log_rows.append({
                    'window_idx': idx + 1,
                    'window_start': str(win_start),
                    'window_end': str(win_end),
                    'threshold': th,
                    'sample_count': selected,
                    'hit_rate': hit_rate
                })

            # 3.4 äººç»´åº¦ç‰¹å¾é‡è¦æ€§ï¼ˆä»¥ç‰¹å¾åå‰ç¼€ pred_ ä½œä¸ºâ€œé¢„æµ‹è€…ç›¸å…³â€ï¼‰
            lgb_top_features_w = train_result_w.get('lgb_top300_features', [])
            lgb_model_w = train_result_w.get('lgb_model', None)
            if lgb_model_w is not None and len(lgb_top_features_w) == len(lgb_model_w.feature_importances_):
                fi_df = pd.DataFrame({
                    'feature': lgb_top_features_w,
                    'importance': lgb_model_w.feature_importances_
                })
                # åªä¿ç•™åƒ pred_123_xxx è¿™ç§å¸¦é¢„æµ‹è€…IDå‰ç¼€çš„ç‰¹å¾ï¼Œé¿å…æŠŠ pred_type_identifier ä¹‹ç±»å…¨å±€ç‰¹å¾ç®—åšäººç‰¹å¾
                human_fi = fi_df[
                    fi_df['feature'].astype(str).str.match(r'^pred_\d+_')
                ].copy()
                if not human_fi.empty:
                    human_fi = human_fi.sort_values('importance', ascending=False).head(15)
                    human_fi['window_idx'] = idx + 1
                    human_imp_list.append(human_fi)

        # 3.x æ±‡æ€»å¯è§†åŒ–
        if len(window_logs) > 0:
            logs_df = pd.DataFrame(window_logs)
            logs_df_sorted = logs_df.sort_values('window_end')

            # 3.x.1 logloss vs æ—¶é—´
            st.markdown("#### ğŸ“‰ å„æ—¶é—´çª—å£ LightGBM éªŒè¯é›† logloss éšæ—¶é—´å˜åŒ–")
            fig, ax = plt.subplots(figsize=(10, 4))
            ax.plot(logs_df_sorted['window_end'], logs_df_sorted['lgb_logloss'], marker='o', linestyle='-')
            ax.set_xlabel('çª—å£ç»“æŸæ—¥æœŸ', fontsize=12)
            ax.set_ylabel('LightGBM éªŒè¯é›† logloss', fontsize=12)
            ax.set_title('logloss vs æ—¶é—´ï¼ˆæŒ‰ betting_cycle_date çª—å£ï¼‰', fontsize=14, fontweight='bold')
            ax.grid(alpha=0.3)
            for i, row in logs_df_sorted.iterrows():
                if not np.isnan(row['lgb_logloss']):
                    ax.text(row['window_end'], row['lgb_logloss'] + 0.005,
                            f"{row['lgb_logloss']:.3f}", ha='center', fontsize=9)
            plt.xticks(rotation=45)
            plt.tight_layout()
            st.pyplot(fig)

            st.markdown("#### ğŸ“Š å„æ—¶é—´çª—å£åŸºç¡€è¡¨ç°æ¦‚è§ˆ")
            st.dataframe(logs_df.round(3), width='stretch')

            # æŒä¹…åŒ–çª—å£çº§æ—¥å¿—
            logs_save_path = os.path.join(
                METRICS_DIR,
                f'window_cv_logs_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv'
            )
            logs_df.to_csv(logs_save_path, index=False, encoding='utf-8-sig')

        if len(bucket_log_rows) > 0:
            bucket_df = pd.DataFrame(bucket_log_rows)
            bucket_df_display = bucket_df.copy()
            def _fmt_hit_rate(x):
                return f"{x * 100:.1f}%" if (x is not None and not np.isnan(x)) else "æ— æ ·æœ¬"
            bucket_df_display['hit_rate'] = bucket_df_display['hit_rate'].apply(_fmt_hit_rate)

            st.markdown("#### ğŸ¯ å„æ—¶é—´çª—å£é«˜ç½®ä¿¡åº¦ bucket çœŸå®å‘½ä¸­ç‡")
            st.dataframe(bucket_df_display, width='stretch')

        if len(human_imp_list) > 0:
            st.markdown("#### ğŸ§ å„æ—¶é—´çª—å£â€œäººç»´åº¦â€ç‰¹å¾é‡è¦æ€§ï¼ˆTop15ï¼Œä»…ç‰¹å¾åå‰ç¼€ pred_ï¼‰")
            for human_df in human_imp_list:
                win_idx = int(human_df['window_idx'].iloc[0])
                # æ‰¾åˆ°å¯¹åº”çª—å£çš„èµ·æ­¢æ—¥æœŸ
                win_row = None
                if 'logs_df' in locals():
                    match_rows = logs_df[logs_df['window_idx'] == win_idx]
                    if not match_rows.empty:
                        win_row = match_rows.iloc[0]
                win_title_suffix = ""
                if win_row is not None:
                    win_title_suffix = f"ï¼ˆ{win_row['window_start']} ~ {win_row['window_end']}ï¼‰"

                fig, ax = plt.subplots(figsize=(8, 6))
                human_plot_df = human_df.sort_values('importance', ascending=True)
                sns.barplot(x='importance', y='feature', data=human_plot_df, ax=ax, color='#e67e22')
                ax.set_xlabel('ç‰¹å¾é‡è¦æ€§å¾—åˆ†', fontsize=12)
                ax.set_ylabel('ç‰¹å¾åç§°', fontsize=10)
                ax.set_title(f'çª—å£ {win_idx} äººç»´åº¦ Top15 ç‰¹å¾{win_title_suffix}', fontsize=14, fontweight='bold')
                ax.grid(axis='x', alpha=0.3)
                plt.tight_layout()
                st.pyplot(fig)

    else:
        st.info("âš ï¸ betting_cycle_date å”¯ä¸€æ—¥æœŸå°‘äº 3 å¤©ï¼Œæš‚ä¸è¿›è¡Œæ—¶é—´çª—å£åˆ‡åˆ†åˆ†æã€‚")

    # ==================== 4. ä½¿ç”¨å…¨é‡æ•°æ®ä»é›¶å¼€å§‹è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰ ====================
    try:
        X, y, feature_cols, current_predictor_ids = feature_engineering(full_df, is_training=True)
        valid_predictors_count = len(current_predictor_ids)
        logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆï¼šæ€»ç‰¹å¾æ•°{len(feature_cols)}ï¼Œæœ‰æ•ˆé¢„æµ‹è€…æ•°{valid_predictors_count}")
    except Exception as e:
        st.error(f"ç‰¹å¾å·¥ç¨‹å¤±è´¥ï¼š{str(e)}")
        return train_history

    st.write(
        f"âœ… å…¨é‡è®­ç»ƒæ•°æ®é‡ï¼š{len(full_df)}æ¡ï¼Œ"
        f"æœ‰æ•ˆé¢„æµ‹è€…æ•°ï¼š{valid_predictors_count}ï¼Œæ€»ç‰¹å¾æ•°ï¼š{len(feature_cols)}"
    )

    current_pred_type = full_df['prediction_type'].iloc[0]
    st.write("ğŸ”§ ä½¿ç”¨å…¨é‡æ•°æ®ä»é›¶å¼€å§‹è®­ç»ƒåŸºç¡€æ¨¡å‹...")
    train_result = train_base_models(X, y, current_predictor_ids, current_pred_type)

    # 5. æ¨¡å‹è¿­ä»£å¯¹æ¯” + ä»…å¯¹æ–°best modelåšå¯è§†åŒ–
    new_lgb_f1 = train_result['metrics']['lgb_metrics']['f1']
    new_lr_f1 = train_result['metrics']['lr_metrics']['f1']
    historical_best_lgb = best_config["lgb_f1"]

    # æ–°ç­–ç•¥ï¼šè€ƒè™‘â€œæ€§èƒ½ + æ—¶æ•ˆæ€§â€çš„åŒé‡çº¦æŸ
    # è§„åˆ™ï¼š
    #   - å¦‚æœè¿˜æ²¡æœ‰å†å²æœ€ä¼˜æ¨¡å‹ï¼ˆhistorical_best_lgb <= 0ï¼‰ï¼Œä»»ä½•æ–°æ¨¡å‹éƒ½è§†ä¸ºæœ€ä¼˜ï¼›
    #   - å¦åˆ™ï¼š
    #       1ï¼‰åªè¦æ–°æ¨¡å‹çš„ F1 æ²¡æœ‰æ¯”å†å²æœ€ä¼˜å·®å¤ªå¤šï¼ˆ<= base_marginï¼‰ï¼Œå°±å…è®¸ç”¨æ–°æ¨¡å‹è¦†ç›–æ—§æ¨¡å‹ï¼›
    #       2ï¼‰å¦‚æœæ–°æ¨¡å‹æ˜æ˜¾æ›´å·®ï¼Œåˆ™æ£€æŸ¥å†å²æœ€ä¼˜æ¨¡å‹æ˜¯å¦â€œè¿‡æ—§â€ï¼ˆè¶…è¿‡ force_refresh_daysï¼‰ï¼Œ
    #          è‹¥è¿‡æ—§åˆ™ä¸ºäº†æ—¶æ•ˆæ€§å¼ºåˆ¶åˆ·æ–°ä¸ºæ–°æ¨¡å‹ã€‚
    base_margin = 0.005        # å…è®¸æ–°æ¨¡å‹æ¯”å†å²æœ€ä¼˜ç•¥ä½çš„æ€§èƒ½å®¹å¿åº¦
    force_refresh_days = 10    # å†å²æœ€ä¼˜æ¨¡å‹å…è®¸â€œä¸è¿‡æœŸâ€çš„å¤©æ•°é˜ˆå€¼

    # é»˜è®¤å‡è®¾æœ¬æ¬¡ä¸æ˜¯æ–°æœ€ä¼˜ï¼Œå¹¶åˆå§‹åŒ– days_gap
    is_new_best = False
    days_gap = None

    if historical_best_lgb <= 0:
        # æ²¡æœ‰å†å²æœ€ä¼˜æ¨¡å‹æ—¶ï¼Œå½“å‰ä¸€å®šæ˜¯æœ€ä¼˜
        is_new_best = True
        days_gap = 0
    else:
        # 1ï¼‰æ€§èƒ½ç»´åº¦ï¼šæ–°æ¨¡å‹æ²¡æœ‰æ¯”å†å²æœ€ä¼˜å·®å¤ªå¤šï¼Œå…è®¸è¦†ç›–
        if new_lgb_f1 >= historical_best_lgb - base_margin:
            is_new_best = True
        else:
            # 2ï¼‰æ—¶æ•ˆç»´åº¦ï¼šå†å²æœ€ä¼˜æ¨¡å‹æ˜¯å¦å·²ç»â€œè¿‡æ—§â€
            old_end_str = best_config.get("window_end", "")
            try:
                old_end = datetime.strptime(old_end_str, "%Y-%m-%d").date()
                if pd.isna(max_date):
                    # è‹¥å½“å‰è®­ç»ƒæ•°æ®æœ€å¤§æ—¥æœŸå¼‚å¸¸ï¼Œåˆ™è§†ä½œææ—§ï¼Œè§¦å‘åˆ·æ–°
                    days_gap = 9999
                else:
                    days_gap = (max_date.date() - old_end).days
            except Exception:
                # window_end ç¼ºå¤±æˆ–è§£æå¤±è´¥æ—¶ï¼Œè§†ä½œææ—§
                days_gap = 9999

            if days_gap is not None and days_gap >= force_refresh_days:
                is_new_best = True
                logger.info(
                    f"å†å²æœ€ä¼˜æ¨¡å‹å·²è¿‡æœŸ {days_gap} å¤©ï¼Œ"
                    f"å³ä½¿æ–°æ¨¡å‹ F1 ç•¥ä½ (new={new_lgb_f1:.3f}, best={historical_best_lgb:.3f})ï¼Œ"
                    f"ä»å¼ºåˆ¶å°†å½“å‰æ¨¡å‹è®¾ä¸ºæ–°çš„æœ€ä¼˜æ¨¡å‹"
                )

    date_tag = max_date.strftime("%Y%m%d") if not pd.isna(max_date) else end_date.replace('-', '')

    if is_new_best or not best_config.get("model_date"):
        # æ›´æ–°æœ€ä¼˜æ¨¡å‹é…ç½®
        new_best_config = {
            "model_date": train_result['model_date'],
            "lgb_f1": new_lgb_f1,
            "lr_f1": new_lr_f1,
            "lgb_path": os.path.join(MODEL_DIR, f'lgb_model_{train_result["model_date"]}.pkl'),
            "lr_path": os.path.join(MODEL_DIR, f'lr_model_{train_result["model_date"]}.pkl'),
            "scaler_path": os.path.join(MODEL_DIR, f'scaler_{train_result["model_date"]}.pkl'),
            "lgb_features_path": os.path.join(MODEL_DIR, f'lgb_features_{train_result["model_date"]}.pkl'),
            "lr_features_path": os.path.join(MODEL_DIR, f'lr_features_{train_result["model_date"]}.pkl'),
            "model_predictors": current_predictor_ids,
            "prediction_type": train_result['prediction_type'],
            "window_end": max_date.strftime("%Y-%m-%d") if not pd.isna(max_date) else end_date
        }
        save_best_model_config(new_best_config)
        best_config = new_best_config
        if historical_best_lgb > 0:
            st.success(
                f"ğŸ‰ æ–°æ¨¡å‹è¢«è®¾ä¸ºæœ€ä¼˜æ¨¡å‹ï¼LightGBM F1 ä» {historical_best_lgb:.3f} "
                f"æ›´æ–°ä¸º {new_lgb_f1:.3f}ï¼ˆå…è®¸ä¸‹é™é˜ˆå€¼ {base_margin:.3f}ï¼‰"
            )
        else:
            st.success(
                f"ğŸ‰ å·²ç”Ÿæˆé¦–ä¸ªæœ€ä¼˜æ¨¡å‹ï¼LightGBM F1 = {new_lgb_f1:.3f}"
            )
    else:
        delta = new_lgb_f1 - historical_best_lgb
        extra_msg = ""
        # å½“ days_gap è¢«æ­£ç¡®è®¡ç®—æ—¶ï¼Œè¡¥å……å±•ç¤ºâ€œè·ç¦»å†å²æœ€ä¼˜çª—å£å·²è¿‡å»å¤šå°‘å¤©â€
        if days_gap is not None:
            extra_msg = f"ï¼Œå½“å‰è®­ç»ƒæ•°æ®ç»“æŸæ—¥æœŸè·ç¦»å†å²æœ€ä¼˜æ¨¡å‹çª—å£ç»“æŸå·²è¿‡å» {days_gap} å¤©"
        st.info(
            f"â„¹ï¸ æ–°æ¨¡å‹æš‚æœªæ›¿æ¢å†å²æœ€ä¼˜ï¼ˆå½“å‰ LightGBM F1: {new_lgb_f1:.3f}ï¼Œ"
            f"å†å²æœ€ä¼˜: {historical_best_lgb:.3f}ï¼Œå·®å€¼ {delta:+.3f}ï¼Œ"
            f"å…è®¸ä¸‹é™é˜ˆå€¼ {base_margin:.3f}{extra_msg}ï¼‰"
        )

    # æ¯æ¬¡è®­ç»ƒéƒ½ç”Ÿæˆå¯è§†åŒ–è¯„ä¼°æŠ¥å‘Šï¼ˆå…¨é‡è§†è§’ï¼‰
    visualize_metrics(train_result, date_tag)

    # 6. æ¸…ç†å†—ä½™æ¨¡å‹æ–‡ä»¶ï¼ˆåªä¿ç•™æœ€ä¼˜ + æœ€è¿‘2ä¸ªï¼‰
    clean_old_models()

    # 7. è®°å½•å…¨é‡è®­ç»ƒå†å²ï¼ˆå•è¡Œï¼Œä¿æŒåŸç»“æ„ï¼‰
    best_model = 'LightGBM' if new_lgb_f1 > new_lr_f1 else 'é€»è¾‘å›å½’'
    history_entry = pd.DataFrame({
        'window_start': [min_date.strftime('%Y-%m-%d') if not pd.isna(min_date) else start_date],
        'window_end': [max_date.strftime('%Y-%m-%d') if not pd.isna(max_date) else end_date],
        'sample_count': [len(full_df)],
        'valid_predictors_count': [len(current_predictor_ids)],
        'lr_f1': [new_lr_f1],
        'lgb_f1': [new_lgb_f1],
        'lr_cv_var': [train_result['metrics']['stability_metrics']['lr_cv_var']],
        'lgb_cv_var': [train_result['metrics']['stability_metrics']['lgb_cv_var']],
        'best_model': [best_model],
        'is_new_best': [1 if is_new_best else 0]
    })
    train_history = pd.concat([train_history, history_entry], ignore_index=True)

    # ä¿å­˜è®­ç»ƒå†å²
    train_history.to_csv(
        os.path.join(METRICS_DIR, f'training_history_{datetime.now().strftime("%Y%m%d")}.csv'),
        index=False,
        encoding='utf-8-sig'
    )

    return train_history

# ===================== 3. å½“æ—¥æ¨ç† =====================
def predict_today(target_date):
    st.markdown(f"### ğŸ¯ å½“æ—¥æ¨ç†ï¼š{target_date}")
    # è§„èŒƒåŒ–æ¨ç†æ—¥æœŸå­—ç¬¦ä¸²ï¼Œåç»­å†™å…¥ç»Ÿè®¡è¡¨æ—¶å¤ç”¨
    target_date_str = str(target_date)
    # æ•°æ®åº“è¡¨åˆå§‹åŒ–æ ¡éªŒ
    if not init_model_pred_tables(DB_PATH):
        st.error("âŒ æ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥")
        return pd.DataFrame(), pd.DataFrame()

    # 1. ä»…ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæ¨ç†
    best_config = load_best_model_config()
    model_dates = []

    # æ¨ç†é˜¶æ®µä¼˜å…ˆä½¿ç”¨â€œæœ€ä½³æ¨¡å‹â€å•æ¨¡å‹è¿›è¡Œé¢„æµ‹
    if best_config.get("model_date"):
        model_dates = [best_config["model_date"]]
    else:
        # è‹¥å°šæœªäº§ç”Ÿæœ€ä½³æ¨¡å‹ï¼Œåˆ™é€€åŒ–ä¸ºä½¿ç”¨æœ€è¿‘ä¸€æ¬¡è®­ç»ƒäº§ç”Ÿçš„æ¨¡å‹
        recent_dates = get_recent_model_dates(top_n=1)
        if recent_dates:
            model_dates = [recent_dates[0]]
            st.warning("âš ï¸ æœªæ‰¾åˆ°æœ€ä¼˜æ¨¡å‹é…ç½®ï¼Œä¸´æ—¶ä½¿ç”¨æœ€è¿‘ä¸€æ¬¡è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†")
        else:
            st.error("âŒ å½“å‰ä¸å­˜åœ¨ä»»ä½•å¯ç”¨æ¨¡å‹ï¼Œè¯·å…ˆå®Œæˆä¸€æ¬¡å…¨é‡è®­ç»ƒ")
            return pd.DataFrame(), pd.DataFrame()

    st.info(f"æœ¬æ¬¡æ¨ç†ä½¿ç”¨çš„æ¨¡å‹æ—¥æœŸï¼š{model_dates[0]}ï¼ˆå•ä¸€æœ€ä½³æ¨¡å‹ï¼‰")

    # 2. åŠ è½½æ¨ç†æ•°æ®
    pred_df = load_prediction_data(DB_PATH, target_date)
    if pred_df.empty:
        st.error("âŒ æœªåŠ è½½åˆ°é¢„æµ‹æ•°æ®ï¼ˆæ£€æŸ¥æ—¥æœŸæˆ–æ•°æ®åº“è¿æ¥ï¼‰")
        return pd.DataFrame(), pd.DataFrame()

    # <--- æ–°å¢ä»£ç ï¼šå…³è” match è¡¨è·å– match_no
    conn = get_db_connection(DB_PATH)
    if conn:
        try:
            # æŸ¥è¯¢ match è¡¨ï¼Œè·å– match_id å’Œ match_no
            match_query = "SELECT match_id, match_no FROM match;"
            match_df = pd.read_sql(match_query, conn)

            # å°† match_no åˆå¹¶åˆ° pred_df ä¸­
            # ä½¿ç”¨å·¦è¿æ¥ï¼Œç¡®ä¿å³ä½¿æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ match_noï¼Œpred_df çš„è®°å½•ä¹Ÿä¸ä¼šä¸¢å¤±
            pred_df = pd.merge(pred_df, match_df, on='match_id', how='left')

            # å¤„ç†å¯èƒ½çš„ç¼ºå¤±å€¼ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            pred_df['match_no'] = pred_df['match_no'].fillna('N/A')

        except Exception as e:
            st.warning(f"âš ï¸ å…³è” match è¡¨è·å–æ¯”èµ›ç¼–å·å¤±è´¥: {e}")
        finally:
            conn.close()

    current_predictor_ids = pred_df['predictor_id'].unique().tolist()
    conn = get_db_connection(DB_PATH)
    if conn:
        all_pids = [p['predictor_id'] for p in conn.execute("SELECT DISTINCT predictor_id FROM predictor").fetchall()]
        current_predictor_ids = list(set(current_predictor_ids + all_pids))
        conn.close()

    # 3. ç‰¹å¾å·¥ç¨‹
    try:
        X, _, feature_cols, _ = feature_engineering(pred_df, is_training=False)
        # <--- æ–°å¢è°ƒè¯•ä»£ç 
        st.subheader("ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šç‰¹å¾çŸ©é˜µ")
        st.write(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
        st.write("ç‰¹å¾çŸ©é˜µå‰5è¡Œ:")
        st.dataframe(X.head())
        # <--- è°ƒè¯•ä»£ç ç»“æŸ
    except Exception as e:
        st.error(f"âŒ ç‰¹å¾å·¥ç¨‹å¤±è´¥ï¼š{str(e)[:150]}")
        return pd.DataFrame(), pd.DataFrame()

    # 4. å¤šæ¨¡å‹æ¨ç†
    model_preds = []

    # <--- å…³é”®ä¿®æ”¹ 1: åœ¨å¾ªç¯å‰åˆå§‹åŒ–å˜é‡ä¸º None
    lgb_model = None
    lgb_features = None
    X_aligned_for_viz = None  # åŒæ ·ä¸ºå¯è§†åŒ–ç”¨çš„ X_aligned åˆå§‹åŒ–

    for idx, model_date in enumerate(model_dates):
        try:
            # åŠ è½½æ¨¡å‹åŠé…å¥—é…ç½®
            temp_lgb_model = joblib.load(os.path.join(MODEL_DIR, f'lgb_model_{model_date}.pkl'))
            lr_model = joblib.load(os.path.join(MODEL_DIR, f'lr_model_{model_date}.pkl'))
            scaler = joblib.load(os.path.join(MODEL_DIR, f'scaler_{model_date}.pkl'))
            temp_lgb_features = joblib.load(os.path.join(MODEL_DIR, f'lgb_features_{model_date}.pkl'))
            lr_features = joblib.load(os.path.join(MODEL_DIR, f'lr_features_{model_date}.pkl'))
            model_predictors = joblib.load(os.path.join(MODEL_DIR, f'model_predictors_{model_date}.pkl'))

            # ç‰¹å¾å¯¹é½
            X_aligned = align_features_with_predictors(
                X=X,
                current_predictor_ids=current_predictor_ids,
                model_predictor_ids=model_predictors,
                model_features=temp_lgb_features
            )

            # ç‰¹å¾ç­›é€‰å’Œç¼©æ”¾
            X_lgb = X_aligned[temp_lgb_features].copy()
            X_lr = X_aligned[lr_features].copy() if all(col in X_aligned.columns for col in lr_features) else X_aligned
            X_lr_scaled = scaler.transform(X_lr).astype('float64')

            # è®¡ç®—ç½®ä¿¡åº¦
            lr_proba = lr_model.predict_proba(X_lr_scaled)[:, 1]
            lgb_proba = temp_lgb_model.predict_proba(X_lgb)[:, 1]

            # <--- æ–°å¢è°ƒè¯•ä»£ç 
            st.write(f"ğŸ“Š æ¨¡å‹ {idx + 1} é¢„æµ‹æ¦‚ç‡:")
            st.write(f"LR æ¨¡å‹æ¦‚ç‡ (å‰10ä¸ª): {lr_proba[:10]}")
            st.write(f"LGB æ¨¡å‹æ¦‚ç‡ (å‰10ä¸ª): {lgb_proba[:10]}")
            # <--- è°ƒè¯•ä»£ç ç»“æŸ

            # çº¿ä¸Šç½®ä¿¡åº¦ï¼šå½“å‰ç‰ˆæœ¬æ”¹ä¸ºçº¯ LightGBM æ¦‚ç‡
            model_confidence = lgb_proba

            model_preds.append(pd.DataFrame({
                'predictor_id': pred_df['predictor_id'].values,
                'match_id': pred_df['match_id'].values,
                'match_no': pred_df['match_no'].values,  # <--- å…³é”®ï¼šåœ¨è¿™é‡ŒåŠ å…¥ match_no
                'betting_cycle_date': pred_df['betting_cycle_date'],
                'original_term': pred_df['original_term'].values,
                'prediction_type': pred_df['prediction_type'].values,
                f'confidence_model_{idx + 1}': model_confidence
            }))
            st.info(f"âœ… åŠ è½½æ¨¡å‹{idx + 1}ï¼ˆæ—¥æœŸï¼š{model_date}ï¼‰æˆåŠŸ")

            # <--- å…³é”®ä¿®æ”¹ 2: åªæœ‰å½“æ¨¡å‹æˆåŠŸåŠ è½½æ—¶ï¼Œæ‰æ›´æ–°ç”¨äºå¯è§†åŒ–çš„å…¨å±€å˜é‡
            # æˆ‘ä»¬ç”¨æœ€åä¸€ä¸ªæˆåŠŸåŠ è½½çš„æ¨¡å‹æ¥åšå¯è§†åŒ–
            lgb_model = temp_lgb_model
            lgb_features = temp_lgb_features
            X_aligned_for_viz = X_aligned

        except Exception as e:
            import traceback
            st.warning(f"âš ï¸ åŠ è½½æ¨¡å‹{idx + 1}ï¼ˆæ—¥æœŸï¼š{model_date}ï¼‰å¤±è´¥ï¼š{str(e)[:100]}")
            st.code(traceback.format_exc()[:500], language='python')
            continue

    # æ ¡éªŒï¼šè‹¥æ— å¯ç”¨æ¨¡å‹ï¼Œç›´æ¥è¿”å›
    if len(model_preds) == 0:
        st.error("âŒ æ— å¯ç”¨æ¨¡å‹å®Œæˆæ¨ç†ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æˆ–è·¯å¾„")
        return pd.DataFrame(), pd.DataFrame()

    # 5. é¢„æµ‹ç»“æœèåˆ
    merged_pred = model_preds[0]
    for p in model_preds[1:]:
        merged_pred = pd.merge(
            merged_pred,
            p,
            on=['predictor_id', 'match_id', 'betting_cycle_date', 'original_term', 'prediction_type', 'match_no'],
            how='outer'
        )
    confidence_cols = [col for col in merged_pred.columns if col.startswith('confidence_model_')]
    merged_pred['confidence'] = merged_pred[confidence_cols].mean(axis=1).round(3)

    # é’ˆå¯¹â€œæ€»è¿›çƒæ•°â€ç©æ³•çš„ç»„åˆæŠ•æ³¨åšä¸€å±‚ç½®ä¿¡åº¦æƒ©ç½šï¼Œé¿å… 2/3çƒã€3/4çƒ ç­‰ä¸è¶…å®½åŒºé—´ç»„åˆæ‹¿åˆ°ç±»ä¼¼ç½®ä¿¡åº¦
    merged_pred = adjust_goal_combo_confidence(merged_pred)

    # æ–°å¢ï¼šä¸º Top2 ç»Ÿè®¡å‡†å¤‡å…¨å±€æœ€é«˜ç½®ä¿¡åº¦çš„å‰2æ¡è®°å½•ï¼ˆè·¨æ¯”èµ›ä¸ç©æ³•ï¼‰
    merged_pred_top2 = (
        merged_pred
        .sort_values('confidence', ascending=False)
        .head(2)
        .copy()
    )

    # 6. å…ˆç®—æ¯åœºæ¯”èµ›ã€æ¯ç§ç©æ³•æœ€ç»ˆé€‰å–çš„æ–¹æ¡ˆï¼ˆåŒä¸€æ¯”èµ›å†…æ¯ç§ç©æ³•å–ç½®ä¿¡åº¦æœ€é«˜çš„ä¸€æ¡ï¼‰
    best_idx = (
        merged_pred
        .groupby(['match_id', 'prediction_type'])['confidence']
        .idxmax()
        .dropna()
        .astype(int)
    )
    match_best_all = merged_pred.loc[best_idx].reset_index(drop=True)

    # æŒ‰ç½®ä¿¡åº¦æ’åºï¼ŒTop10 ç”¨äºåé¢çš„å›¾è¡¨ + å¡ç‰‡å±•ç¤º
    match_best_pred = match_best_all.sort_values('confidence', ascending=False).head(10)

    # æ—¥æœŸå±•ç¤ºå­—æ®µ
    match_best_all['display_date'] = match_best_all['betting_cycle_date'].dt.strftime('%Y-%m-%d')
    match_best_pred['display_date'] = match_best_pred['betting_cycle_date'].dt.strftime('%Y-%m-%d')

    # åˆå¹¶ç›˜å£ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'handicap_value' in pred_df.columns:
        match_info = pred_df[['match_id', 'handicap_value']].drop_duplicates('match_id')
        match_best_all = pd.merge(match_best_all, match_info, on='match_id', how='left')
        match_best_pred = pd.merge(match_best_pred, match_info, on='match_id', how='left')

    # å¡«é»˜è®¤å€¼
    match_best_all = match_best_all.fillna({
        'handicap_value': 'æ— '
    })
    match_best_pred = match_best_pred.fillna({
        'handicap_value': 'æ— '
    })

    # 7. æ–°å¢ï¼šé¢„æµ‹å¯è§£é‡Šæ€§å¯è§†åŒ–
    st.markdown("### ğŸ“‹ å…¨éƒ¨æ¯”èµ›æœ€ç»ˆé¢„æµ‹ï¼ˆæ¯åœºæ¯ç©æ³•å–ç½®ä¿¡åº¦æœ€é«˜çš„æ–¹æ¡ˆï¼‰")

    all_display_cols = ['display_date', 'match_no', 'prediction_type', 'original_term', 'confidence']
    existing_cols = [c for c in all_display_cols if c in match_best_all.columns]

    st.dataframe(
        match_best_all[existing_cols].sort_values('confidence', ascending=False).reset_index(drop=True),
        width='stretch'
    )

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle(f'é¢„æµ‹åˆ†ææŠ¥å‘Šï¼ˆ{target_date}ï¼‰', fontsize=16, fontweight='bold', y=0.98)

    # 7.1 Top10é¢„æµ‹ç½®ä¿¡åº¦æ’åº
    y_labels = [f"æ¯”èµ›#{row['match_id']}" for _, row in match_best_pred.iterrows()]
    axes[0, 0].barh(range(len(match_best_pred)), match_best_pred['confidence'], color='#e74c3c', alpha=0.8)
    axes[0, 0].set_yticks(range(len(match_best_pred)))
    axes[0, 0].set_yticklabels(y_labels, fontsize=10)
    axes[0, 0].set_xlabel('ç½®ä¿¡åº¦', fontsize=12)
    axes[0, 0].set_title('Top10é¢„æµ‹ç½®ä¿¡åº¦æ’åº', fontweight='bold')
    axes[0, 0].axvline(x=0.8, color='green', linestyle='--', alpha=0.8, label='é«˜ç½®ä¿¡é˜ˆå€¼ï¼ˆ0.8ï¼‰')
    axes[0, 0].legend()
    axes[0, 0].grid(axis='x', alpha=0.3)

    # 7.2 ç½®ä¿¡åº¦åˆ†å¸ƒ
    axes[0, 1].hist(
        match_best_pred['confidence'],
        bins=5,
        color='#3498db',
        alpha=0.8,
        edgecolor='black'
    )
    axes[0, 1].set_xlabel('ç½®ä¿¡åº¦åŒºé—´', fontsize=12)
    axes[0, 1].set_ylabel('é¢„æµ‹æ•°é‡', fontsize=12)
    axes[0, 1].set_title('Top10é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ', fontweight='bold')
    axes[0, 1].grid(axis='y', alpha=0.3)

    # 7.3 Top10 ç½®ä¿¡åº¦èµ°åŠ¿ï¼ˆå½“å‰ç‰ˆæœ¬ä»…ä½¿ç”¨å•æ¨¡å‹ï¼Œä¸å†å±•ç¤ºæ¨¡å‹å…±è¯†åº¦ï¼‰
    axes[1, 0].plot(
        range(1, len(match_best_pred) + 1),
        match_best_pred['confidence'],
        marker='o',
        linestyle='-',
        linewidth=2,
        alpha=0.8
    )
    axes[1, 0].set_xlabel('Top æ’åï¼ˆ1 = æœ€é«˜ç½®ä¿¡åº¦ï¼‰', fontsize=12)
    axes[1, 0].set_ylabel('ç½®ä¿¡åº¦', fontsize=12)
    axes[1, 0].set_title('Top10 ç½®ä¿¡åº¦èµ°åŠ¿', fontweight='bold')
    axes[1, 0].axhline(y=0.8, color='green', linestyle='--', alpha=0.7, label='é«˜ç½®ä¿¡é˜ˆå€¼ï¼ˆ0.8ï¼‰')
    axes[1, 0].legend()
    axes[1, 0].grid(alpha=0.3)

    # <--- å…³é”®ä¿®æ”¹ 3: åœ¨ä½¿ç”¨ lgb_model å’Œ lgb_features ä¹‹å‰ï¼Œå…ˆæ£€æŸ¥å®ƒä»¬æ˜¯å¦ä¸º None
    if lgb_model is not None and lgb_features is not None and X_aligned_for_viz is not None:
        # 7.4 æœ€é«˜ç½®ä¿¡åº¦é¢„æµ‹çš„æ ¸å¿ƒç‰¹å¾è´¡çŒ®
        top_pred = match_best_pred.iloc[0]
        # ä½¿ç”¨ä¸ºå¯è§†åŒ–ä¿å­˜çš„ X_aligned_for_viz
        X_top = X_aligned_for_viz[X_aligned_for_viz.index == top_pred.name][lgb_features].copy()
        feature_importance = pd.DataFrame({
            'feature': lgb_features[:10],
            'importance': lgb_model.feature_importances_[:10]
        }).sort_values('importance', ascending=True)
        axes[1, 1].barh(
            feature_importance['feature'],
            feature_importance['importance'],
            color='#2ecc71',
            alpha=0.8
        )
        axes[1, 1].set_xlabel('ç‰¹å¾é‡è¦æ€§å¾—åˆ†', fontsize=12)
        axes[1, 1].set_title(f'æœ€é«˜ç½®ä¿¡åº¦é¢„æµ‹ï¼ˆæ¯”èµ›#{top_pred["match_id"]}ï¼‰æ ¸å¿ƒç‰¹å¾', fontweight='bold')
        axes[1, 1].grid(axis='x', alpha=0.3)
    else:
        # å¦‚æœå˜é‡æœªè¢«æ­£ç¡®åˆå§‹åŒ–ï¼Œåˆ™åœ¨ç¬¬å››ä¸ªå­å›¾ä¸Šæ˜¾ç¤ºæç¤ºä¿¡æ¯
        axes[1, 1].text(0.5, 0.5, 'æ— æ³•åŠ è½½LGBæ¨¡å‹ä»¥ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾',
                        horizontalalignment='center',
                        verticalalignment='center',
                        transform=axes[1, 1].transAxes,
                        fontsize=14,
                        color='red')
        axes[1, 1].set_xlim(0, 1)
        axes[1, 1].set_ylim(0, 1)
        axes[1, 1].axis('off')  # å…³é—­åæ ‡è½´

    # è°ƒæ•´å¸ƒå±€+å±•ç¤ºå›¾è¡¨
    plt.tight_layout(rect=[0, 0, 1, 0.96])
    st.pyplot(fig)


    # 7.x æ–°å¢ï¼šè‡ªåŠ¨å…¥åº“æ¨¡å‹é¢„æµ‹æ˜ç»†ï¼ˆæ¯åœºæ¯ç©æ³•æœ€ä½³ + å…¨å±€Top2ï¼‰
    try:
        conn = get_db_connection(DB_PATH)
        if conn:
            cursor = conn.cursor()

            def _insert_model_record(row):
                """å°†ä¸€æ¡æ¨¡å‹é¢„æµ‹ç»“æœå†™å…¥ model_prediction_recordsï¼ˆè‹¥å·²å­˜åœ¨åˆ™å¿½ç•¥ï¼‰"""
                pred_dt = row.get('betting_cycle_date')
                # å…¼å®¹ Timestamp / datetime / å­—ç¬¦ä¸²
                if pd.isna(pred_dt):
                    pred_date_str = str(target_date)
                else:
                    if hasattr(pred_dt, 'strftime'):
                        pred_date_str = pred_dt.strftime('%Y-%m-%d')
                    else:
                        pred_date_str = str(pred_dt)[:10]

                cursor.execute(
                    """
                    INSERT OR IGNORE INTO model_prediction_records
                    (pred_date, match_id, original_term, prediction_type, confidence)
                    VALUES (?, ?, ?, ?, ?)
                    """,
                    (
                        pred_date_str,
                        int(row['match_id']),
                        str(row['original_term']),
                        str(row['prediction_type']),
                        float(row['confidence'])
                    )
                )

            # 7.x.1 æ¯åœºæ¯”èµ›ã€æ¯ç§ç©æ³•ç½®ä¿¡åº¦æœ€é«˜çš„æ–¹æ¡ˆ
            for _, r in match_best_all.iterrows():
                _insert_model_record(r)

            # 7.x.2 å½“å¤©å…¨å±€ç½®ä¿¡åº¦æœ€é«˜çš„ Top2 æ–¹æ¡ˆï¼ˆè·¨æ¯”èµ› + è·¨ç©æ³•ï¼‰
            for _, r in merged_pred_top2.iterrows():
                _insert_model_record(r)

            # 7.x.3 è®°å½•å½“æ—¥å…¨å±€ç½®ä¿¡åº¦æœ€é«˜çš„ Top2 é¢„æµ‹åˆ° model_pred_stats_top2 æ˜ç»†è¡¨
            # è¡¨ç»“æ„åœ¨ utils.init_model_pred_tables ä¸­å®šä¹‰ï¼ŒåŒ…å«ï¼špred_date, match_id,
            # prediction_type, original_term, confidence, bucket_name ç­‰å­—æ®µï¼Œ
            # ç”¨äºæ¯å¤©ä»…ä¿ç•™å…¨å±€ Top2 é¢„æµ‹æ˜ç»†ï¼ˆä¸å†å­˜ predictor_idï¼‰ã€‚
            try:
                for _, r in merged_pred_top2.iterrows():
                    # ä½¿ç”¨ target_date_str ä½œä¸ºé¢„æµ‹æ—¥æœŸé”®ï¼Œç¡®ä¿â€œä¸€å¤©åªä¿ç•™å…¨å±€ Top2â€
                    cursor.execute(
                        """
                        INSERT OR REPLACE INTO model_pred_stats_top2
                        (pred_date, match_id, prediction_type, original_term, confidence, bucket_name)
                        VALUES (?, ?, ?, ?, ?, ?)
                        """,
                        (
                            target_date_str,
                            int(r['match_id']),
                            str(r['prediction_type']),
                            str(r['original_term']),
                            float(r['confidence']),
                            'TOP2'
                        )
                    )
            except Exception as e:
                st.warning(f"âš ï¸ å†™å…¥ model_pred_stats_top2 Top2 è®°å½•å¤±è´¥ï¼š{str(e)[:150]}")

            conn.commit()
            st.success("âœ… å·²å°†å½“æ—¥æ¨¡å‹é¢„æµ‹æ˜ç»†è‡ªåŠ¨å†™å…¥ model_prediction_recordsï¼ˆå»é‡åï¼‰")
    except Exception as e:
        st.warning(f"âš ï¸ è‡ªåŠ¨å†™å…¥æ¨¡å‹é¢„æµ‹æ˜ç»†å¤±è´¥ï¼š{str(e)[:150]}")
    finally:
        if 'conn' in locals() and conn:
            conn.close()

    # 8. Top10é¢„æµ‹å±•ç¤ºï¼ˆå¸¦â€œåŠ å…¥è®°å½•â€æŒ‰é’®ï¼‰
    # ... (è¿™éƒ¨åˆ†ä»£ç æ²¡æœ‰é—®é¢˜ï¼Œæ­¤å¤„çœç•¥)
    st.markdown("### ğŸ† å½“æ—¥Top10é«˜ç½®ä¿¡åº¦é¢„æµ‹")

    def get_confidence_style(conf):
        if conf >= 0.8:
            return 'background-color: #d4edda; color: #155724; font-weight: bold'
        elif conf >= 0.6:
            return 'background-color: #fff3cd; color: #856404; font-weight: bold'
        else:
            return 'background-color: #f8d7da; color: #721c24; font-weight: bold'

    for idx, (_, row) in enumerate(match_best_pred.iterrows()):
        cols = st.columns([1.2, 1, 2, 2, 1.4])
        with cols[0]:
            st.write(f"ğŸ“… {row['display_date']}")
        with cols[1]:
            st.write(f"#{row['match_no']}")
        with cols[2]:
            st.write(row['prediction_type'])
        with cols[3]:
            st.write(row['original_term'])
        with cols[4]:
            st.markdown(
                f"<div style='{get_confidence_style(row['confidence'])}; padding: 4px; border-radius: 4px'>{row['confidence']:.3f}</div>",
                unsafe_allow_html=True
            )
        st.markdown("---")

    # 9. æ¨¡å‹å†å²è¡¨ç°ï¼ˆåŸºäº Top2 è®°å½•ï¼‰
    st.markdown("### ğŸ“ˆ æ¨¡å‹å†å²è¡¨ç°ï¼ˆTop2 å‘½ä¸­ç‡ï¼‰")

    total_rows = 0
    labeled_rows = 0
    hit_rows = 0

    conn = get_db_connection(DB_PATH)
    if conn:
        try:
            cursor = conn.cursor()
            # è¡¨æ€»è®°å½•æ•°
            cursor.execute("SELECT COUNT(*) FROM model_pred_stats_top2")
            row = cursor.fetchone()
            total_rows = row[0] if row and row[0] is not None else 0

            # is_hit å­—æ®µå·²å¡«å†™çš„è®°å½•æ•°ï¼ˆä¸ä¸º NULLï¼‰
            cursor.execute("SELECT COUNT(*) FROM model_pred_stats_top2 WHERE is_hit IS NOT NULL")
            row = cursor.fetchone()
            labeled_rows = row[0] if row and row[0] is not None else 0

            # å‘½ä¸­è®°å½•æ•°ï¼ˆis_hit = 1ï¼‰
            cursor.execute("SELECT COUNT(*) FROM model_pred_stats_top2 WHERE is_hit = 1")
            row = cursor.fetchone()
            hit_rows = row[0] if row and row[0] is not None else 0

        except Exception as e:
            st.warning(f"âš ï¸ ç»Ÿè®¡ Top2 å‘½ä¸­ç‡å¤±è´¥ï¼š{str(e)[:150]}")
        finally:
            conn.close()

    stat_cols = st.columns(3)
    with stat_cols[0]:
        st.metric("Top2 å·²ç»“ç®—æ¬¡æ•°", labeled_rows, delta=0, help="is_hit å­—æ®µä¸ä¸º NULL çš„è®°å½•æ•°ï¼ˆå·²ç»“ç®—ï¼‰")
    with stat_cols[1]:
        st.metric("Top2 å‘½ä¸­æ¬¡æ•°", hit_rows, delta=0, help="is_hit = 1 çš„è®°å½•æ•°ï¼ˆå‘½ä¸­æ¬¡æ•°ï¼‰")
    with stat_cols[2]:
        acc = hit_rows / labeled_rows if labeled_rows > 0 else 0.0
        st.metric("Top2 å‘½ä¸­ç‡ï¼ˆå·²ç»“ç®—ï¼‰", f"{acc:.1%}", delta=0, help="å‘½ä¸­æ¬¡æ•° / å·²ç»“ç®—æ¬¡æ•°ï¼ˆis_hit=1 / is_hitä¸ä¸ºNULLï¼‰")

    # 10. å¯é€‰ï¼šè‡ªåŠ¨å…¥åº“Top2é«˜ç½®ä¿¡é¢„æµ‹
    # ... (è¿™éƒ¨åˆ†ä»£ç æ²¡æœ‰é—®é¢˜ï¼Œæ­¤å¤„çœç•¥)
    # if len(match_best_pred) >= 2:
    #     auto_top2 = match_best_pred.head(2)
    #     if save_prediction_to_db(DB_PATH, auto_top2):
    #         st.success(f"âœ… å·²è‡ªåŠ¨å°†Top2é«˜ç½®ä¿¡é¢„æµ‹å­˜å…¥æ•°æ®åº“")
    #     else:
    #         st.warning("âš ï¸ è‡ªåŠ¨å…¥åº“Top2é¢„æµ‹å¤±è´¥ï¼Œå¯æ‰‹åŠ¨ç‚¹å‡»â€œåŠ å…¥è®°å½•â€è¡¥å½•")
    # else:
    #     st.warning("âš ï¸ å¯ç”¨é¢„æµ‹ä¸è¶³2æ¡ï¼Œè·³è¿‡è‡ªåŠ¨å…¥åº“")

    return match_best_pred, pd.DataFrame()

# ===================== 4. Streamlitå¯è§†åŒ–è¯„ä¼°ï¼ˆé€‚é…å…¨é‡ç‰¹å¾ï¼‰ =====================
def visualize_metrics(train_result, date_tag):
    """å¯è§†åŒ–åŸºç¡€æ€§èƒ½+ç‰¹å¾é‡è¦æ€§+ç‰¹å¾é€‰æ‹©ç»“æœ"""
    lr_metrics = train_result['metrics']['lr_metrics']
    lgb_metrics = train_result['metrics']['lgb_metrics']
    stability_metrics = train_result['metrics']['stability_metrics']
    y_test = train_result['y_test']
    lr_pred = train_result['lr_pred']
    lgb_pred = train_result['lgb_pred']
    lgb_proba = train_result['lgb_proba']
    lr_proba = train_result.get('lr_proba', None)
    ensemble_proba = train_result.get('ensemble_proba', None)
    feature_cols = train_result['metrics']['feature_cols']
    lgb_top300_features = train_result.get('lgb_top300_features', [])
    lr_final_features = train_result.get('lr_final_features', [])


    # åˆ›å»º4x2å­å›¾
    fig, axes = plt.subplots(4, 2, figsize=(18, 22))
    fig.suptitle(f'æ¨¡å‹è¯„ä¼°æŠ¥å‘Šï¼ˆ{date_tag}ï¼‰', fontsize=18, fontweight='bold', y=0.98)

    # ---------------------- 1. åŸºç¡€æ€§èƒ½æŒ‡æ ‡å¯¹æ¯” ----------------------
    metrics_names = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1å€¼']
    lr_values = [lr_metrics['accuracy'], lr_metrics['precision'], lr_metrics['recall'], lr_metrics['f1']]
    lgb_values = [lgb_metrics['accuracy'], lgb_metrics['precision'], lgb_metrics['recall'], lgb_metrics['f1']]

    x = np.arange(len(metrics_names))
    width = 0.35
    axes[0, 0].bar(x - width / 2, lr_values, width, label='é€»è¾‘å›å½’', color='#3498db', alpha=0.8)
    axes[0, 0].bar(x + width / 2, lgb_values, width, label='LightGBM', color='#e74c3c', alpha=0.8)
    axes[0, 0].set_xlabel('æŒ‡æ ‡ç±»å‹', fontsize=12)
    axes[0, 0].set_ylabel('æ•°å€¼', fontsize=12)
    axes[0, 0].set_title('åŸºç¡€æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    axes[0, 0].set_xticks(x)
    axes[0, 0].set_xticklabels(metrics_names)
    axes[0, 0].legend()
    axes[0, 0].grid(axis='y', alpha=0.3)
    # æ ‡æ³¨æ•°å€¼
    for i, (lr_val, lgb_val) in enumerate(zip(lr_values, lgb_values)):
        axes[0, 0].text(i - width / 2, lr_val + 0.01, f'{lr_val:.3f}', ha='center', fontsize=10)
        axes[0, 0].text(i + width / 2, lgb_val + 0.01, f'{lgb_val:.3f}', ha='center', fontsize=10)

    # ---------------------- 2. LightGBMæ··æ·†çŸ©é˜µ ----------------------
    cm = np.array(lgb_metrics['confusion_matrix'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1], cbar_kws={'label': 'é¢„æµ‹æ–¹æ¡ˆæ•°'})
    axes[0, 1].set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    axes[0, 1].set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    axes[0, 1].set_title('é¢„æµ‹æ–¹æ¡ˆå‘½ä¸­æƒ…å†µæ··æ·†çŸ©é˜µï¼ˆ0=æœªå‘½ä¸­ï¼Œ1=å‘½ä¸­ï¼‰', fontsize=14, fontweight='bold')
    axes[0, 1].set_xticklabels(['æœªå‘½ä¸­', 'å‘½ä¸­'])
    axes[0, 1].set_yticklabels(['æœªå‘½ä¸­', 'å‘½ä¸­'])

    # ---------------------- 3. ç¨³å®šæ€§æŒ‡æ ‡ ----------------------
    model_names = ['é€»è¾‘å›å½’', 'LightGBM']
    cv_vars = [stability_metrics['lr_cv_var'], stability_metrics['lgb_cv_var']]
    cv_means = [stability_metrics['lr_cv_f1_mean'], stability_metrics['lgb_cv_f1_mean']]

    bars = axes[1, 0].bar(model_names, cv_vars, color=['#3498db', '#e74c3c'], alpha=0.8)
    axes[1, 0].set_xlabel('æ¨¡å‹ç±»å‹', fontsize=12)
    axes[1, 0].set_ylabel('F1å€¼æ–¹å·®ï¼ˆè¶Šå°è¶Šç¨³å®šï¼‰', fontsize=12)
    axes[1, 0].set_title('5æŠ˜äº¤å‰éªŒè¯ç¨³å®šæ€§', fontsize=14, fontweight='bold')
    axes[1, 0].grid(axis='y', alpha=0.3)
    axes[1, 0].axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='ç¨³å®šé˜ˆå€¼ï¼ˆ0.1ï¼‰')
    axes[1, 0].legend()
    # æ ‡æ³¨æ•°å€¼
    for bar, var, mean in zip(bars, cv_vars, cv_means):
        height = bar.get_height()
        axes[1, 0].text(bar.get_x() + bar.get_width() / 2., height + 0.001,
                        f'æ–¹å·®ï¼š{var:.3f}\nå‡å€¼F1ï¼š{mean:.3f}', ha='center', fontsize=10)

    # ---------------------- 3.1 æ¦‚ç‡å±‚é¢çš„ logloss å¯¹æ¯”ï¼ˆLGB vs Ensembleï¼‰ ----------------------
    logloss_rows = []

    try:
        lgb_logloss = log_loss(y_test, lgb_proba)
        logloss_rows.append({'æ¨¡å‹': 'LightGBM', 'logloss': lgb_logloss})
    except Exception:
        lgb_logloss = None

    if ensemble_proba is not None:
        try:
            ens_logloss = log_loss(y_test, ensemble_proba)
            logloss_rows.append({'æ¨¡å‹': 'Ensemble(LGB+LR)/2', 'logloss': ens_logloss})
        except Exception:
            pass

    if lr_proba is not None:
        try:
            lr_logloss = log_loss(y_test, lr_proba)
            logloss_rows.append({'æ¨¡å‹': 'Logistic Regression', 'logloss': lr_logloss})
        except Exception:
            pass

    if len(logloss_rows) > 0:
        logloss_df = pd.DataFrame(logloss_rows)
        st.markdown("#### ğŸ“‰ æ¦‚ç‡å±‚é¢çš„ logloss å¯¹æ¯”ï¼ˆéªŒè¯é›†ï¼‰")
        st.dataframe(logloss_df.round(4), width='stretch')

    # ---------------------- 4. é¢„æµ‹åˆ†å¸ƒå¯¹æ¯” ----------------------
    axes[1, 1].hist(lr_pred, bins=2, alpha=0.6, label='é€»è¾‘å›å½’', color='#3498db', density=True, rwidth=0.7)
    axes[1, 1].hist(lgb_pred, bins=2, alpha=0.6, label='LightGBM', color='#e74c3c', density=True, rwidth=0.7)
    axes[1, 1].hist(y_test, bins=2, alpha=0.4, label='çœŸå®ç»“æœ', color='#2ecc71', density=True, rwidth=0.7)
    axes[1, 1].set_xlabel('æ ‡ç­¾ï¼ˆ0=æœªå‘½ä¸­ï¼Œ1=å‘½ä¸­ï¼‰', fontsize=12)
    axes[1, 1].set_ylabel('å¯†åº¦', fontsize=12)
    axes[1, 1].set_title('é¢„æµ‹æ–¹æ¡ˆåˆ†å¸ƒä¸çœŸå®ç»“æœå¯¹æ¯”', fontsize=14, fontweight='bold')
    axes[1, 1].set_xticks([0, 1])
    axes[1, 1].set_xticklabels(['æœªå‘½ä¸­', 'å‘½ä¸­'])
    axes[1, 1].legend()

    # ---------------------- 5. LightGBM Top20ç‰¹å¾é‡è¦æ€§ ----------------------
    if len(lgb_top300_features) > 0 and hasattr(train_result['lgb_model'], 'feature_importances_'):
        feature_importance = pd.DataFrame({
            'feature': lgb_top300_features[:20],
            'importance': train_result['lgb_model'].feature_importances_[:20]
        }).sort_values('importance', ascending=True)
        sns.barplot(x='importance', y='feature', data=feature_importance, ax=axes[2, 0], color='#e74c3c')
        axes[2, 0].set_xlabel('ç‰¹å¾é‡è¦æ€§å¾—åˆ†', fontsize=12)
        axes[2, 0].set_ylabel('ç‰¹å¾åç§°', fontsize=10)
        axes[2, 0].set_title('LightGBM Top20æ ¸å¿ƒç‰¹å¾ï¼ˆåŸºäºå…¨éƒ¨ç‰¹å¾ï¼‰', fontsize=14, fontweight='bold')
        axes[2, 0].grid(axis='x', alpha=0.3)

    # ---------------------- 6. PRæ›²çº¿+æŠ•æ³¨é˜ˆå€¼é€‰æ‹© ----------------------
    from sklearn.metrics import precision_recall_curve, average_precision_score
    precision, recall, thresholds = precision_recall_curve(y_test, lgb_proba)
    average_precision = average_precision_score(y_test, lgb_proba)

    axes[2, 1].plot(recall, precision, color='#e74c3c', lw=2, label=f'å¹³å‡ç²¾ç¡®ç‡ï¼ˆAPï¼‰={average_precision:.3f}')
    axes[2, 1].set_xlabel('å¬å›ç‡ï¼ˆé¢„æµ‹æ–¹æ¡ˆå‘½ä¸­è¦†ç›–ç‡ï¼‰', fontsize=12)
    axes[2, 1].set_ylabel('ç²¾ç¡®ç‡ï¼ˆé¢„æµ‹æ–¹æ¡ˆå®é™…å‘½ä¸­ç‡ï¼‰', fontsize=12)
    axes[2, 1].set_title('é¢„æµ‹æ–¹æ¡ˆPRæ›²çº¿ä¸æŠ•æ³¨é˜ˆå€¼é€‰æ‹©', fontsize=14, fontweight='bold')
    axes[2, 1].legend()
    axes[2, 1].grid(alpha=0.3)

    # æ ‡æ³¨æ ¸å¿ƒé˜ˆå€¼ï¼ˆç²¾ç¡®ç‡â‰¥0.8ï¼‰
    target_precision = 0.8
    closest_idx = np.argmin(np.abs(precision - target_precision))
    best_threshold = 0.1
    if closest_idx < len(thresholds):
        best_threshold = thresholds[closest_idx]
        axes[2, 1].scatter(recall[closest_idx], precision[closest_idx], color='red', s=80, zorder=5)
        axes[2, 1].annotate(
            f'æ¨èé˜ˆå€¼={best_threshold:.3f}\nç²¾ç¡®ç‡={precision[closest_idx]:.3f}\nè¦†ç›–ç‡={recall[closest_idx]:.3f}',
            xy=(recall[closest_idx], precision[closest_idx]),
            xytext=(recall[closest_idx] + 0.1, precision[closest_idx] - 0.1),
            arrowprops=dict(arrowstyle='->', color='red', lw=2)
        )

    # ---------------------- 6.1 é«˜ç½®ä¿¡åº¦åŒºé—´çœŸå®å‘½ä¸­ç‡ç»Ÿè®¡ï¼ˆLGB vs Ensembleï¼‰ ----------------------
    # åœ¨éªŒè¯é›†ä¸Šç»Ÿè®¡ä¸åŒé˜ˆå€¼ä¸‹çš„æ ·æœ¬æ•°å’ŒçœŸå®å‘½ä¸­ç‡ï¼Œå¸®åŠ©åˆ¤æ–­â€œåªæŠ¼é«˜ç½®ä¿¡åº¦æ–¹æ¡ˆâ€çš„å®æˆ˜ä»·å€¼
    y_array = np.asarray(y_test)
    high_thresholds = [0.6, 0.7, 0.8, 0.9]

    def collect_bucket_stats(proba, name):
        rows = []
        for th in high_thresholds:
            mask = proba >= th
            selected = int(mask.sum())
            if selected > 0:
                hit_rate = float(y_array[mask].mean())
            else:
                hit_rate = np.nan
            rows.append({
                'æ¨¡å‹': name,
                'é˜ˆå€¼': th,
                'æ ·æœ¬æ•°': selected,
                'çœŸå®å‘½ä¸­ç‡': hit_rate if not np.isnan(hit_rate) else None
            })
        return rows

    all_rows = []
    # LightGBM å¿…å¡«
    all_rows += collect_bucket_stats(lgb_proba, 'LightGBM')

    # Ensemble å¯é€‰
    if ensemble_proba is not None:
        all_rows += collect_bucket_stats(ensemble_proba, 'Ensemble(LGB+LR)/2')

    # LR æ¦‚ç‡ï¼ˆæ›´å¤šæ˜¯ sanity checkï¼‰
    if lr_proba is not None:
        all_rows += collect_bucket_stats(lr_proba, 'Logistic Regression')

    if len(all_rows) > 0:
        stats_df = pd.DataFrame(all_rows)
        stats_df_display = stats_df.copy()
        stats_df_display['çœŸå®å‘½ä¸­ç‡'] = stats_df_display['çœŸå®å‘½ä¸­ç‡'].apply(
            lambda x: f"{x * 100:.1f}%" if x is not None else "æ— æ ·æœ¬"
        )
        st.markdown("#### ğŸ¯ é«˜ç½®ä¿¡åº¦åŒºé—´çœŸå®å‘½ä¸­ç‡ï¼ˆéªŒè¯é›† LGB vs Ensemble å¯¹æ¯”ï¼‰")
        st.dataframe(stats_df_display, width='stretch')

    # ---------------------- 7. ç‰¹å¾é€‰æ‹©ç»“æœå¯¹æ¯” ----------------------
    feature_count_data = pd.DataFrame({
        'æ¨¡å‹': ['å…¨éƒ¨ç‰¹å¾', 'Logistic Regressionæœ€ç»ˆ'],
        'ç‰¹å¾æ•°é‡': [len(feature_cols), len(lr_final_features)]
    })
    sns.barplot(
        x='æ¨¡å‹',
        y='ç‰¹å¾æ•°é‡',
        data=feature_count_data,
        ax=axes[3, 0],
        palette=['#95a5a6', '#3498db']
    )
    axes[3, 0].set_xlabel('ç‰¹å¾é˜¶æ®µ', fontsize=12)
    axes[3, 0].set_ylabel('ç‰¹å¾æ•°é‡', fontsize=12)
    axes[3, 0].set_title('ç‰¹å¾é€‰æ‹©è¿‡ç¨‹å¯¹æ¯”', fontsize=14, fontweight='bold')
    axes[3, 0].grid(axis='y', alpha=0.3)
    for i, count in enumerate(feature_count_data['ç‰¹å¾æ•°é‡']):
        axes[3, 0].text(i, count + 1, str(count), ha='center', fontsize=12, fontweight='bold')

    # ---------------------- 8. é€»è¾‘å›å½’æ ¸å¿ƒç‰¹å¾ï¼ˆTop10ï¼‰ ----------------------
    if len(lr_final_features) > 0 and hasattr(train_result['lr_model'], 'coef_'):
        lr_coef = pd.DataFrame({
            'feature': lr_final_features[:10],
            'coef_abs': np.abs(train_result['lr_model'].coef_[0][:10])
        }).sort_values('coef_abs', ascending=True)
        sns.barplot(x='coef_abs', y='feature', data=lr_coef, ax=axes[3, 1], color='#3498db')
        axes[3, 1].set_xlabel('ç³»æ•°ç»å¯¹å€¼ï¼ˆé‡è¦æ€§ï¼‰', fontsize=12)
        axes[3, 1].set_ylabel('ç‰¹å¾åç§°', fontsize=10)
        axes[3, 1].set_title('Logistic Regression Top10æ ¸å¿ƒç‰¹å¾', fontsize=14, fontweight='bold')
        axes[3, 1].grid(axis='x', alpha=0.3)

    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout(rect=[0, 0, 1, 0.96])

    # ä¿å­˜+æ˜¾ç¤º
    save_path = os.path.join(VIS_DIR, f'metrics_{date_tag}.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    st.pyplot(fig)

    # è®¡ç®—ä¸€ä¸ªå…¸å‹é«˜ç½®ä¿¡é˜ˆå€¼ï¼ˆ0.8ï¼‰ä¸‹çš„å‘½ä¸­ç‡ï¼Œç”¨äºæ€»ç»“
    high_mask = lgb_proba >= 0.8
    high_sample_cnt = int(high_mask.sum())
    high_hit_rate = float(np.asarray(y_test)[high_mask].mean()) if high_sample_cnt > 0 else None

    # æ ¸å¿ƒç»“è®º
    st.markdown("### ğŸ“‹ æ ¸å¿ƒè¯„ä¼°ç»“è®º")
    st.markdown(
        f"- **LightGBM**ï¼šF1å€¼ **{lgb_metrics['f1']}**ï¼Œç¨³å®šæ€§æ–¹å·® **{stability_metrics['lgb_cv_var']}**ï¼ˆ{'åˆæ ¼' if stability_metrics['lgb_cv_var'] < 0.1 else 'éœ€ä¼˜åŒ–'}ï¼‰ï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾ï¼ˆå…±{len(lgb_top300_features)}ä¸ªï¼‰")
    st.markdown(
        f"- **é€»è¾‘å›å½’**ï¼šF1å€¼ **{lr_metrics['f1']}**ï¼Œç¨³å®šæ€§æ–¹å·® **{stability_metrics['lr_cv_var']}**ï¼ˆ{'åˆæ ¼' if stability_metrics['lr_cv_var'] < 0.1 else 'éœ€ä¼˜åŒ–'}ï¼‰ï¼Œä½¿ç”¨å»ç›¸å…³åçš„ç‰¹å¾ï¼ˆ{len(lr_final_features)}ä¸ªï¼‰")
    st.markdown(f"- **æœ€ä¼˜æ¨¡å‹**ï¼š**{('LightGBM' if lgb_metrics['f1'] > lr_metrics['f1'] else 'é€»è¾‘å›å½’')}**")
    st.markdown(f"- **æ¨èæŠ•æ³¨é˜ˆå€¼**ï¼š**{best_threshold:.3f}**ï¼ˆå¯¹åº”ç²¾ç¡®ç‡â‰¥{target_precision}ï¼‰")

    if high_hit_rate is not None:
        st.markdown(f"- **é«˜ç½®ä¿¡åº¦åŒºé—´ï¼ˆé˜ˆå€¼â‰¥0.8ï¼‰**ï¼šæ ·æœ¬æ•° **{high_sample_cnt}**ï¼ŒçœŸå®å‘½ä¸­ç‡ **{high_hit_rate:.1%}**")
    else:
        st.markdown(f"- **é«˜ç½®ä¿¡åº¦åŒºé—´ï¼ˆé˜ˆå€¼â‰¥0.8ï¼‰**ï¼šå½“å‰éªŒè¯é›†æ— æ ·æœ¬ï¼Œæš‚æ— æ³•è¯„ä¼°å‘½ä¸­ç‡")

    st.markdown(f"- **æ ¸å¿ƒç‰¹å¾ç¤ºä¾‹**ï¼šLightGBMTop1={lgb_top300_features[0] if len(lgb_top300_features) > 0 else 'æ— '}ï¼ŒLRTop1={lr_final_features[0] if len(lr_final_features) > 0 else 'æ— '}")

# ===================== 5. Streamlitä¸»ç•Œé¢ =====================
def main():
    st.set_page_config(page_title="ç«å½©é¢„æµ‹æ¨¡å‹è®­ç»ƒç³»ç»Ÿ", layout="wide")
    st.title("âš½ ç«å½©é¢„æµ‹æ¨¡å‹è®­ç»ƒä¸æ¨ç†ç³»ç»Ÿ")
    st.markdown("---")

    # ä¾§è¾¹æ é…ç½®
    st.sidebar.header("ğŸ“‹ åŠŸèƒ½é€‰æ‹©")
    function_option = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½", ["å…¨é‡è®­ç»ƒ", "å½“æ—¥æ¨ç†"])

    # åˆå§‹åŒ–Session State
    if 'train_history' not in st.session_state:
        st.session_state['train_history'] = pd.DataFrame()

    # ---------------------- åŠŸèƒ½1ï¼šå…¨é‡è®­ç»ƒ ----------------------
    if function_option == "å…¨é‡è®­ç»ƒ":
        st.header("ğŸš€ å…¨é‡è®­ç»ƒé…ç½®")
        env_label = "ç”Ÿäº§ç¯å¢ƒ" if CURRENT_ENV == "prod" else "å¼€å‘ç¯å¢ƒ"
        st.caption(f"å½“å‰ç¯å¢ƒï¼š{env_label}ï¼ˆFOOTBALL_ENV={CURRENT_ENV}ï¼‰")

        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input("è®­ç»ƒèµ·å§‹æ—¥æœŸ", value=pd.to_datetime('2025-10-11'))
        with col2:
            default_end_date = (datetime.now() - timedelta(days=1)).date()
            end_date = st.date_input("è®­ç»ƒç»“æŸæ—¥æœŸ", value=default_end_date)

        st.markdown("---")

        if st.button("å¯åŠ¨æ¨¡å‹è®­ç»ƒ", type="primary", width='stretch'):
            if start_date > end_date:
                st.error("âŒ èµ·å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ")
                return

            with st.spinner("ğŸ”„ æ­£åœ¨æ‰§è¡Œå…¨é‡è®­ç»ƒ... è¯·è€å¿ƒç­‰å¾…"):
                train_history = train_global_model(
                    start_date=start_date.strftime('%Y-%m-%d'),
                    end_date=end_date.strftime('%Y-%m-%d')
                )

            st.session_state['train_history'] = train_history

            if not train_history.empty:
                st.markdown("### ğŸ“ˆ è®­ç»ƒå†å²æ±‡æ€»")
                st.dataframe(train_history.round(3), width='stretch')

                # è®­ç»ƒè¶‹åŠ¿å›¾ï¼ˆå³ä½¿ç›®å‰æ¯æ¬¡åªæœ‰ä¸€è¡Œï¼Œä¹Ÿä¿ç•™ï¼Œæ–¹ä¾¿åç»­æ‰©å±•ï¼‰
                fig, ax = plt.subplots(figsize=(12, 6))
                ax.plot(train_history['window_end'], train_history['lr_f1'], marker='o', label='é€»è¾‘å›å½’F1',
                        color='#3498db')
                ax.plot(train_history['window_end'], train_history['lgb_f1'], marker='s', label='LightGBM F1',
                        color='#e74c3c')
                ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='åˆæ ¼çº¿ï¼ˆ0.5ï¼‰')
                ax.set_xlabel('è®­ç»ƒæ•°æ®ç»“æŸæ—¥æœŸ')
                ax.set_ylabel('F1å€¼')
                ax.set_title('å…¨é‡è®­ç»ƒF1å€¼è¶‹åŠ¿')
                ax.legend()
                ax.grid(alpha=0.3)
                st.pyplot(fig)

    # ---------------------- åŠŸèƒ½2ï¼šå½“æ—¥æ¨ç† ----------------------
    elif function_option == "å½“æ—¥æ¨ç†":
        st.header("ğŸ¯ å½“æ—¥æ¨ç†é…ç½®")
        env_label = "ç”Ÿäº§ç¯å¢ƒ" if CURRENT_ENV == "prod" else "å¼€å‘ç¯å¢ƒ"
        st.caption(f"å½“å‰ç¯å¢ƒï¼š{env_label}ï¼ˆFOOTBALL_ENV={CURRENT_ENV}ï¼‰")

        target_date = st.date_input("é€‰æ‹©æ¨ç†æ—¥æœŸ", value=pd.to_datetime(datetime.now().strftime('%Y-%m-%d')))
        target_date_str = target_date.strftime('%Y-%m-%d')

        st.markdown("---")

        if st.button("æ‰§è¡Œå½“æ—¥æ¨ç†", type="primary", width='stretch'):
            with st.spinner("ğŸ” æ­£åœ¨æ‰§è¡Œæ¨ç†..."):
                match_best_pred, two_combos = predict_today(target_date_str)

    # ---------------------- ä¾§è¾¹æ ä¿¡æ¯ ----------------------
    st.sidebar.markdown("---")
    st.sidebar.info(f"""
    ğŸ“Œ ç¯å¢ƒé…ç½®ï¼š{CURRENT_ENV}
    ğŸ“ æ•°æ®åº“è·¯å¾„ï¼š{DB_PATH}
    ğŸ“Š å·²è®­ç»ƒæ¨¡å‹æ•°ï¼š{len(get_recent_model_dates(top_n=10))}
    ğŸ“ˆ ç‰¹å¾é…ç½®ï¼šé¢„æµ‹è€…-æ–¹æ¡ˆå¯¹è”åˆç‰¹å¾ + è¾…åŠ©ç‰¹å¾
    ğŸ¯ æ¨¡å‹æ¶æ„ï¼šLightGBMï¼ˆå…¨éƒ¨ç‰¹å¾ï¼‰+ Logistic Regressionï¼ˆå»ç›¸å…³åç‰¹å¾ï¼‰
    ğŸ”§ æ¨ç†ç­–ç•¥ï¼šä»…æœ€ä¼˜æ¨¡å‹ï¼ˆå•æ¨¡å‹æ¨ç†ï¼‰
    """)

if __name__ == "__main__":
    main()